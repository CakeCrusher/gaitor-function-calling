{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ecd4127-e0e3-44a3-909c-e31833de9260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/sosa.s/.cache/huggingface/datasets/json/default-966e646397bf13ea/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data using FC: 850\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "relative_path_to_data = '../data/chat/production_train_chat-instruction.json'\n",
    "\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "filtered_data = [item for item in dataset if \"</FUNCTION_CALL_ARGUMENTS>\" in item[\"text\"][-33:]]\n",
    "print(\"Amount of data using FC:\", len(filtered_data))\n",
    "\n",
    "df = pd.DataFrame(filtered_data)\n",
    "filtered_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7857ba60-d9c4-46c4-90e3-1232975ecb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7cfbcecade4a1c8132d19a3087426a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "\n",
    "checkpoint_path = \"/home/sosa.s/gaitor-function-calling/instruction_tune/llama-7-chat-instruction-int4-fc-nat/checkpoint-117\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b3f8c4e-4c0e-4b17-a721-a9cf4b61780b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYour job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. \\n<FUNCTIONS>[{\"name\": \"getHashtagFollowerCount\", \"description\": \"Get the follower count, follower count over time and related trending hashtags of a specified LinkedIn hashtag\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path_params\": {\"type\": \"object\", \"properties\": {\"hashtag\": {\"type\": \"string\", \"description\": \"The specified hashtag.\"}}, \"required\": [\"hashtag\"]}}}}]</FUNCTIONS>\\n<</SYS>>\\n\\nlinkedin hashtags for tech [/INST] <FUNCTION_CALL_NAME>fetchImage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"params\": {\"query\": \"dog\"}}</FUNCTION_CALL_ARGUMENTS> </s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28913bcc-24aa-4bb5-8abc-6e7a4dcab98c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] <<SYS>>\n",
      "Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. \n",
      "<FUNCTIONS>[{\"name\": \"getHashtagFollowerCount\", \"description\": \"Get the follower count, follower count over time and related trending hashtags of a specified LinkedIn hashtag\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path_params\": {\"type\": \"object\", \"properties\": {\"hashtag\": {\"type\": \"string\", \"description\": \"The specified hashtag.\"}}, \"required\": [\"hashtag\"]}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "linkedin hashtags for tech [/INST]\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "NST] <FUNCTION_CALL_NAME>getTopNews</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"params\": {\"country\": \"us\", \"language\": \"en\", \"category\": \"tech\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n",
      "\n",
      "\n",
      "Ground truth:\n",
      " <FUNCTION_CALL_NAME>fetchImage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"params\": {\"query\": \"dog\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "sample = filtered_dataset[1][\"text\"]\n",
    "\n",
    "inp, target = sample.split(\"[/INST]\")\n",
    "prompt = inp + \"[/INST]\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy())[0][len(prompt):]}\\n\\n\")\n",
    "print(f\"Ground truth:\\n{target}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
