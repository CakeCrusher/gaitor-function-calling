# instruction_tune
Instruction-tune Llama 2 for function-calling.

### Step 1: Create JSON-Formatted Instruction-Output Pairs
Synthetically generate pairs (10000) of instructions and the desired output in JSON format.

**Example Pair 1:**
```json
{
  "instruction": "Generate a JSON response for a function call based on the user's question and the function call example.",
  "input": {
    "user_question": "What is the weather like in Boston?",
    "function_call_example": {
      "name": "get_current_weather",
      "arguments": {
        "location": "Boston, MA",
        "unit": "fahrenheit"
      }
    }
  },
  "output": {
    "function_call": {
      "name": "get_current_weather",
      "arguments": {
        "location": "Boston, MA",
        "unit": "fahrenheit"
      }
    }
  }
}
```

**Example Pair 2:**
```json
{
  "instruction": "Generate a JSON response for a function call based on the user's question and the function call example.",
  "input": {
    "user_question": "Convert 100 USD to EUR",
    "function_call_example": {
      "name": "currency_conversion",
      "arguments": {
        "amount": 100,
        "from": "USD",
        "to": "EUR"
      }
    }
  },
  "output": {
    "function_call": {
      "name": "currency_conversion",
      "arguments": {
        "amount": 100,
        "from": "USD",
        "to": "EUR"
      }
    }
  }
}
```

### Step 2: Preprocess Data
Ensure JSON-formatted instruction-output pairs are consistent and correctly formatted. 

### Step 3: Fine-Tune the Model
1. **Upload Data**: Save JSON-formatted instruction-output pairs in a file and upload it to Hi-Per-Gator.
2. **Configure Fine-Tuning**: Set the required parameters for fine-tuning.
3. **Run Fine-Tuning**: Start the fine-tuning process.

### Step 4: Test the Model
After the fine-tuning process, rigorously test the model to ensure it generates accurate and reliable JSON responses.

### Step 5: Iterate to Improve
If the performance is not satisfactory, adjust instruction-output pairs, reprocess data, or tweak the fine-tuning parameters, and then re-run the process.

### Next Step:
1. Determine how to encode input data for training
2. Choose valid hyperparameters

## References
- [Fine tune with PEFT](https://huggingface.co/blog/llama2#fine-tuning-with-peft)
- [Instruction tune llama 2](https://www.philschmid.de/instruction-tune-llama-2)
  - instruction tuning according to this paper is using the model response as the input and the instruction(prompt) as the target.
- several different ways to to fine tune a model: prompt tuning, 
- qlora reduces the number of parameters to be fine-tuned by using a small number of parameters to represent the prompt and the response.
- [2 stage fine tuning with soft prompts](https://arxiv.org/pdf/2211.00635.pdf)
  - prompt tuning
    - soft prompts
      - adapt the model to respond how you want to train
      - generated by the ai model
        - using the embeddings as a basis for creating the prompt
        - passing the soft prompt, input, and target to the model.
- [fine tuning llama 2](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications)
  - adding tokens to the tokenizer could improve training performance
  - ![perplexity score curve](https://images.ctfassets.net/xjan103pcp94/1NHkYacCqQEDmHOx3fGJyt/f86fc3eff57937c027ae17f45bee2b9a/Llama_2_learning_curve.png)
    - over time the perplexity score evaluation begins to increase
  - If sharding is necessary we can leverage [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    - A single A100 might be sufficient to fine tune Llama 2 7B
- [Llama 2 text embeddings](https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d)
- [Instruction fine-tuning Llama 2 with PEFTâ€™s QLoRa](https://ukey.co/blog/finetune-llama-2-peft-qlora-huggingface/)
