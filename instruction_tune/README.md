# instruction_tune
Instruction-tune Llama 2 for function-calling.

## References
- [Fine tune with PEFT](https://huggingface.co/blog/llama2#fine-tuning-with-peft)
- [Instruction tune llama 2](https://www.philschmid.de/instruction-tune-llama-2)
  - instruction tuning according to this paper is using the model response as the input and the instruction(prompt) as the target.
- several different ways to to fine tune a model: prompt tuning, 
- qlora reduces the number of parameters to be fine-tuned by using a small number of parameters to represent the prompt and the response.
- [2 stage fine tuning with soft prompts](https://arxiv.org/pdf/2211.00635.pdf)
  - prompt tuning
    - soft prompts
      - adapt the model to respond how you want to train
      - generated by the ai model
        - using the embeddings as a basis for creating the prompt
        - passing the soft prompt, input, and target to the model.
- [fine tuning llama 2](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications)
  - adding tokens to the tokenizer could improve training performance
  - ![perplexity score curve](https://images.ctfassets.net/xjan103pcp94/1NHkYacCqQEDmHOx3fGJyt/f86fc3eff57937c027ae17f45bee2b9a/Llama_2_learning_curve.png)
    - over time the perplexity score evaluation begins to increase
  - If sharding is necessary we can leverage [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    - A single A100 might be sufficient to fine tune Llama 2 7B