{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ddf7b6-1b32-4347-845a-b046ed1162b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.34.0 in /home/sosa.s/.local/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: datasets==2.13.0 in /home/sosa.s/.local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: peft==0.4.0 in /home/sosa.s/.local/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate==0.23.0 in /home/sosa.s/.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: bitsandbytes==0.41.1 in /home/sosa.s/.local/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: trl==0.4.7 in /home/sosa.s/.local/lib/python3.10/site-packages (0.4.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sosa.s/.local/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: ipywidgets in /home/sosa.s/.local/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: wandb in /home/sosa.s/.local/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.34.0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.34.0) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.13.0) (14.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from datasets==2.13.0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.13.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from datasets==2.13.0) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from datasets==2.13.0) (3.8.6)\n",
      "Requirement already satisfied: psutil in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from peft==0.4.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from peft==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipywidgets) (8.13.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /home/sosa.s/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/sosa.s/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from wandb) (1.34.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/sosa.s/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/sosa.s/.local/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: six>=1.4.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.5.0)\n",
      "Requirement already satisfied: backcall in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors>=0.3.1\" ipywidgets wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bfb4a8-35ab-4678-9e06-661b72b075c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function_calling_tokens = {\n",
    "    \"FUNCTIONS\": {\n",
    "        \"start\": \"<FUNCTIONS>\",\n",
    "        \"end\": \"</FUNCTIONS>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_NAME\": {\n",
    "        \"start\": \"<FUNCTION_CALL_NAME>\",\n",
    "        \"end\": \"</FUNCTION_CALL_NAME>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_ARGUMENTS\": {\n",
    "        \"start\": \"<FUNCTION_CALL_ARGUMENTS>\",\n",
    "        \"end\": \"</FUNCTION_CALL_ARGUMENTS>\"\n",
    "    },\n",
    "    \"all\": [\"<FUNCTIONS>\", \"</FUNCTIONS>\", \"<FUNCTION_CALL_NAME>\", \"</FUNCTION_CALL_NAME>\", \"<FUNCTION_CALL_ARGUMENTS>\", \"</FUNCTION_CALL_ARGUMENTS>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d59154-6372-4024-b656-64c4f4774c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/sosa.s/.cache/huggingface/datasets/json/default-7aee0b44caf3693f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "1 sentence about https://www.openplugin.io/ [/INST] <FUNCTION_CALL_NAME>transcodeWebPage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"link\": \"https://www.openplugin.io/\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "relative_path_to_data = '../data/chat/production_train_chat.json'\n",
    "\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(dataset[30][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc307d-c0bf-4364-9128-5f8febe96bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a82de-331c-448c-bd42-75c3ed84f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.spatial.distance import cosine\n",
    "import json\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs[0][0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    # Evaluate JSON structure\n",
    "    structure_score = evaluate_json_structure(generated_json, expected_json)\n",
    "    \n",
    "    # Evaluate sentence similarity for string values\n",
    "    string_similarity_scores = []\n",
    "    for key, value in expected_json.items():\n",
    "        if isinstance(value, str):\n",
    "            gen_value = generated_json.get(key, \"\")\n",
    "            sim_score = sentence_similarity(value, gen_value)\n",
    "            string_similarity_scores.append(sim_score)\n",
    "    \n",
    "    avg_string_similarity = sum(string_similarity_scores) / len(string_similarity_scores) if string_similarity_scores else 1\n",
    "    # Combine structure score and string similarity\n",
    "    final_score = (structure_score + avg_string_similarity) / 2\n",
    "    return final_score\n",
    "\n",
    "# Example usage\n",
    "generated_json = json.loads(model_output)\n",
    "expected_json = json.loads(expected_output)\n",
    "score = custom_metric(generated_json, expected_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7cb2e8f-6cf2-4c65-acbf-171b38cc5da3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data using FC: 850\n"
     ]
    }
   ],
   "source": [
    "filtered_data = [item for item in dataset if \"<FUNCTION_CALL_NAME>\" in item[\"text\"]]\n",
    "print(\"Amount of data using FC:\", len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a4f17fa-5460-46a0-9ba5-9ba94bf0a086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ninja in /home/sosa.s/.local/lib/python3.10/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: packaging in /apps/pytorch/2.0.1/lib/python3.10/site-packages (23.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flash-attn in /home/sosa.s/.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: torch in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from flash-attn) (2.0.1)\n",
      "Requirement already satisfied: einops in /home/sosa.s/.local/lib/python3.10/site-packages (from flash-attn) (0.7.0)\n",
      "Requirement already satisfied: packaging in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from flash-attn) (23.1)\n",
      "Requirement already satisfied: ninja in /home/sosa.s/.local/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch->flash-attn) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\n",
      "Requirement already satisfied: sympy in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=1 pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9239e5c3-3a84-4c7f-a3cb-4d36731e84ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "# model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\" # gated\n",
    "\n",
    "output_dir = \"llama-7-chat-int4-fc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1434b81-e914-4673-92d3-64e9eda7d101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7deea94daf542d4875b52b2a32e1a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre add tokens: 32000\n",
      "Post add tokens: 32006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32006, 4096)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    use_flash_attention_2=use_flash_attention,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"Pre add tokens:\", len(tokenizer))\n",
    "tokenizer.add_tokens(function_calling_tokens[\"all\"])\n",
    "print(\"Post add tokens:\", len(tokenizer))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47c436bd-faaa-4cc9-8590-770acdddc5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e4413b8-dcae-45d4-853e-45f29af63a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3efbba5e-a016-4734-ae1e-117c08ee06f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    # formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53a78685-085b-41a6-b33d-2e1cb393d938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastiansosa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d49df4c-f26a-45fe-9c8e-a4283316f280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lxx2l0dd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-wood-2</strong> at: <a href='https://wandb.ai/sebastiansosa/fc_chat_prod/runs/lxx2l0dd' target=\"_blank\">https://wandb.ai/sebastiansosa/fc_chat_prod/runs/lxx2l0dd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231113_184453-lxx2l0dd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lxx2l0dd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf13578689594edcb66e0bf765c133dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111978801070815, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sosa.s/gaitor-function-calling/instruction_tune/wandb/run-20231113_184623-8rkdgmqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sebastiansosa/fc_chat_prod/runs/8rkdgmqb' target=\"_blank\">comfy-eon-3</a></strong> to <a href='https://wandb.ai/sebastiansosa/fc_chat_prod' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sebastiansosa/fc_chat_prod' target=\"_blank\">https://wandb.ai/sebastiansosa/fc_chat_prod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sebastiansosa/fc_chat_prod/runs/8rkdgmqb' target=\"_blank\">https://wandb.ai/sebastiansosa/fc_chat_prod/runs/8rkdgmqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sebastiansosa/fc_chat_prod/runs/8rkdgmqb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1494e9058700>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='fc_chat_prod', entity='sebastiansosa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e33f185-3c09-4da0-b170-aedf6390155c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1407, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.8568, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.7505, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.7268, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
      "{'loss': 0.6347, 'learning_rate': 0.0002, 'epoch': 1.1}\n",
      "{'loss': 0.6548, 'learning_rate': 0.0002, 'epoch': 1.16}\n",
      "{'loss': 0.5633, 'learning_rate': 0.0002, 'epoch': 2.03}\n",
      "{'loss': 0.542, 'learning_rate': 0.0002, 'epoch': 2.09}\n",
      "{'loss': 0.4787, 'learning_rate': 0.0002, 'epoch': 2.15}\n",
      "{'loss': 0.541, 'learning_rate': 0.0002, 'epoch': 3.01}\n",
      "{'loss': 0.4412, 'learning_rate': 0.0002, 'epoch': 3.07}\n",
      "{'loss': 0.4484, 'learning_rate': 0.0002, 'epoch': 3.13}\n",
      "{'loss': 0.4279, 'learning_rate': 0.0002, 'epoch': 3.19}\n",
      "{'loss': 0.3807, 'learning_rate': 0.0002, 'epoch': 4.06}\n",
      "{'loss': 0.4359, 'learning_rate': 0.0002, 'epoch': 4.12}\n",
      "{'loss': 0.4135, 'learning_rate': 0.0002, 'epoch': 4.18}\n",
      "{'loss': 0.3498, 'learning_rate': 0.0002, 'epoch': 5.04}\n",
      "{'loss': 0.3848, 'learning_rate': 0.0002, 'epoch': 5.1}\n",
      "{'loss': 0.3864, 'learning_rate': 0.0002, 'epoch': 5.16}\n",
      "{'train_runtime': 1596.8212, 'train_samples_per_second': 5.136, 'train_steps_per_second': 0.643, 'train_loss': 0.5495707255143386, 'epoch': 5.19}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "# 4:30 mins per epoch\n",
    "# 10:00 min per 2 epoch\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "326c4667-17b9-4db4-9385-f02521d0de61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "trainer_state.json\n",
      "README.md\n",
      "optimizer.pt\n",
      "scheduler.pt\n",
      "adapter_model.bin\n",
      "rng_state.pth\n",
      "tokenizer_config.json\n",
      "training_args.bin\n",
      "adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'your_directory_path' with the path of the directory you want to list\n",
    "directory_path = '/home/sosa.s/gaitor-function-calling/instruction_tune/llama-7-int4-fc/checkpoint-39'\n",
    "\n",
    "# List all files and directories in the specified path\n",
    "for filename in os.listdir(directory_path):\n",
    "    if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b4313bb-0c66-465f-b42e-57198ab8b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated successfully.\n",
      "checkpoint_path:  /home/sosa.s/gaitor-function-calling/instruction_tune/llama-7-chat-int4-fc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8d9d06dc5e478d9a4c4cf7677eb0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "checkpoint_path = \"/home/sosa.s/gaitor-function-calling/instruction_tune/llama-7-chat-int4-fc\"\n",
    "\n",
    "adapter_config_path = checkpoint_path + '/adapter_config.json'\n",
    "try:\n",
    "    with open(adapter_config_path, 'r') as file:\n",
    "        config_data = json.load(file)\n",
    "\n",
    "    # Update the 'base_model_name_or_path' field\n",
    "    config_data['base_model_name_or_path'] = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "    # Write the updated data back to the file\n",
    "    with open(adapter_config_path, 'w') as file:\n",
    "        json.dump(config_data, file, indent=4)\n",
    "\n",
    "    print(\"File updated successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {adapter_config_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"checkpoint_path: \", checkpoint_path)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d8e90d-68d5-4bbc-8c59-78f32f68632d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] <<SYS>>\n",
      "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "1 sentence about https://www.openplugin.io/ [/INST]\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "NST] OpenPlugin.io is a platform that enables developers to create plugins for various applications, providing them with a seamless way to extend the functionality of their applications.</s>\n",
      "\n",
      "\n",
      "Ground truth:\n",
      " <FUNCTION_CALL_NAME>transcodeWebPage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"link\": \"https://www.openplugin.io/\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "sample = dataset[30][\"text\"]\n",
    "\n",
    "inp, target = sample.split(\"[/INST]\")\n",
    "prompt = inp + \"[/INST]\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy())[0][len(prompt):]}\\n\\n\")\n",
    "print(f\"Ground truth:\\n{target}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c249ea2-b014-4f74-b5b7-841c6d743a71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baebe50bef194b8cb1b365a1092aa735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88f3f1ed-b33c-406c-a384-f2b977334650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dea372da754ff98363e7c3bc4cd128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SebastianS/function_calling-llama_7b/commit/b1d3d8f68fcb986ce4de3c0cb068e45cf6106ddc', commit_message='Upload tokenizer', commit_description='', oid='b1d3d8f68fcb986ce4de3c0cb068e45cf6106ddc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = \"function_calling-llama_7b\"\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af81385c-c38d-4842-b5f3-f8237095ff9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c69c8c8be7545149b2662ea1f7fd92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "hub_id = \"SebastianS/function_calling-llama_7b\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    hub_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(hub_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e951428-f29f-4f6a-aa9a-f0375fcaac95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] <<SYS>>\n",
      "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "\"In two sentences tell me what this site is about https://www.openplugin.io/\" [/INST]\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "NST] OpenPlugin.io is a website that offers a tool for users to interact with non-URL strings, providing them with tailored search results and connecting them to the internet for up-to-date information.</s>\n",
      "\n",
      "\n",
      "Ground truth:\n",
      " <FUNCTION_CALL_NAME>transcodeWebPage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"link\": \"https://www.openplugin.io/\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub and get a sample\n",
    "sample = dataset[29][\"text\"]\n",
    "\n",
    "inp, target = sample.split(\"[/INST]\")\n",
    "prompt = inp + \"[/INST]\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy())[0][len(prompt):]}\\n\\n\")\n",
    "print(f\"Ground truth:\\n{target}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf0a0b2d-be4b-4f15-b863-7add95900c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_str = \"\"\"<s>[INST] <<SYS>>\n",
    "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
    "<</SYS>>\n",
    "\n",
    "1 sentence about https://www.openplugin.io/ [/INST] <FUNCTION_CALL_NAME>transcodeWebPage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"link\": \"https://www.openplugin.io/\"}}</FUNCTION_CALL_ARGUMENTS> </s>\"\"\"\n",
    "expected_str_target = expected_str.split(\"[/INST]\")[1]\n",
    "generated_str = \"\"\"<s>[INST] <<SYS>>\n",
    "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
    "<</SYS>>\n",
    "\n",
    "1 sentence about https://www.openplugin.io/ [/INST] <FUNCTION_CALL_NAME>transcodeWebPage</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"links\": \"https://www.openplugin.io/\"}}</FUNCTION_CALL_ARGUMENTS> </s>\"\"\"\n",
    "generated_str_target = generated_str.split(\"[/INST]\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d929a821-4ccb-4f3a-a010-b962654edef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def parse_prompt_back_to_data(prompt):\n",
    "    \"\"\"\n",
    "    Function to parse a prompt back into the original data format, using a dictionary of function calling tokens.\n",
    "    \n",
    "    :param prompt: A string representing the constructed prompt.\n",
    "    :param function_calling_tokens: A dictionary containing the start and end tokens for different function call elements.\n",
    "    :return: A dictionary representing the original data instance.\n",
    "    \"\"\"\n",
    "    # Building regular expression patterns using the function_calling_tokens\n",
    "    functions_pattern = rf\"{function_calling_tokens['FUNCTIONS']['start']}(.*?){function_calling_tokens['FUNCTIONS']['end']}\"\n",
    "    input_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"  # This remains unchanged as it's not part of function_calling_tokens\n",
    "    target_content_pattern = r\"\\[/INST\\] (.*)</s>\"  # This also remains unchanged\n",
    "    function_call_name_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_NAME']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_NAME']['end']}\"\n",
    "    function_call_arguments_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}\"\n",
    "\n",
    "    # Extracting data using regular expressions\n",
    "    functions_str = re.search(functions_pattern, prompt).group(1)\n",
    "    input_content = re.search(input_pattern, prompt).group(1)\n",
    "    target_content_match = re.search(target_content_pattern, prompt)\n",
    "\n",
    "    # Parse functions JSON string\n",
    "    functions = json.loads(functions_str)\n",
    "\n",
    "    # Prepare the data dictionary\n",
    "    data = {\n",
    "        \"input\": [{\n",
    "            \"chatgptMessage\": {\"role\": \"user\", \"content\": input_content},\n",
    "            \"functions\": functions\n",
    "        }],\n",
    "        \"target\": {\n",
    "            \"chatgptMessage\": {\"role\": \"assistant\"},\n",
    "            \"functions\": functions  # Including functions in the target as well\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the target has a function call\n",
    "    if function_calling_tokens['FUNCTION_CALL_NAME']['start'] in prompt:\n",
    "        function_call_name = re.search(function_call_name_pattern, prompt).group(1)\n",
    "        function_call_arguments = re.search(function_call_arguments_pattern, prompt).group(1)\n",
    "        data[\"target\"][\"chatgptMessage\"][\"function_call\"] = {\n",
    "            \"name\": function_call_name,\n",
    "            \"arguments\": function_call_arguments\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where regex might not find a match for target content\n",
    "        if target_content_match:\n",
    "            target_content = target_content_match.group(1)\n",
    "            data[\"target\"][\"chatgptMessage\"][\"content\"] = target_content\n",
    "\n",
    "    return data\n",
    "expected_data = parse_prompt_back_to_data(expected_str)\n",
    "generated_data = parse_prompt_back_to_data(generated_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bcb9804-fadd-4a1d-aacf-f3ab1dbd1b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9852805435657501"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import json\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "embedding_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embedding_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "\n",
    "    return mean_pooled[0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    def compare_json(g_json, e_json, key_similarity_scores, value_similarity_scores):\n",
    "        for e_key, e_value in e_json.items():\n",
    "            # Check for exact key match or find the most similar key\n",
    "            if e_key in g_json:\n",
    "                g_key = e_key\n",
    "                key_similarity_scores.append(1)\n",
    "            else:\n",
    "                # Compute similarity with all keys in generated_json and find the best match\n",
    "                key_similarity = {gen_key: sentence_similarity(e_key, gen_key) for gen_key in g_json.keys()}\n",
    "                g_key, key_sim_score = max(key_similarity.items(), key=lambda x: x[1])\n",
    "                key_similarity_scores.append(key_sim_score)\n",
    "\n",
    "            # Recursive comparison for nested objects, else compare values\n",
    "            if isinstance(e_value, dict) and isinstance(g_json.get(g_key, {}), dict):\n",
    "                compare_json(g_json[g_key], e_value, key_similarity_scores, value_similarity_scores)\n",
    "            elif isinstance(e_value, str) and isinstance(g_json.get(g_key, \"\"), str):\n",
    "                # Compare values only if they are strings at the root level\n",
    "                value_sim_score = sentence_similarity(e_value, g_json[g_key])\n",
    "                value_similarity_scores.append(value_sim_score)\n",
    "            elif e_value == g_json.get(g_key, None):\n",
    "                value_similarity_scores.append(1)  # Exact match for non-string root values\n",
    "            else:\n",
    "                value_similarity_scores.append(0)  # Non-matching root values\n",
    "\n",
    "    key_similarity_scores = []\n",
    "    value_similarity_scores = []\n",
    "    compare_json(generated_json, expected_json, key_similarity_scores, value_similarity_scores)\n",
    "\n",
    "    # Compute the average similarity scores\n",
    "    avg_key_similarity = sum(key_similarity_scores) / len(key_similarity_scores) if key_similarity_scores else 0\n",
    "    avg_value_similarity = sum(value_similarity_scores) / len(value_similarity_scores) if value_similarity_scores else 0\n",
    "\n",
    "    return (avg_key_similarity + avg_value_similarity) / 2\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generated_json = json.loads(generated_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "expected_json = json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "score = custom_metric(generated_json, expected_json)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc70649f-380f-4cfd-b0cf-759c88b7d999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'json': {'link': 'https://www.openplugin.io/'}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab2dc10-7c3c-429a-99e3-fbb53b4d921c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/SebastianS/function_calling-llama_7b', endpoint='https://huggingface.co', repo_type='model', repo_id='SebastianS/function_calling-llama_7b')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "import os\n",
    "api = HfApi()\n",
    "api.create_repo(\"function_calling-llama_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b7b5d-b460-47ca-9f0e-26b1a20ee28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repository(\"function_calling-llama_7b\", clone_from=\"SebastianS/function_calling-llama_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57a724d-065d-4bba-a7cc-0336206793cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m args\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-7-int4-fc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# load base LLM model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     model_id,\n\u001b[1;32m     15\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[1;32m     16\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention,\n\u001b[1;32m     18\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "args.output_dir = \"llama-7-int4-fc\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    use_flash_attention_2=use_flash_attention,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1cd522f-c54b-46a9-8d89-f9d16997dc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "  Below is an instruction that describes a \"function calling\" task...\n",
      "\n",
      "  ### Instruction:\n",
      "  Analyze the prompt and json speicifaction pair (denoted in <FC></FC>) to produce a relevant \"function calling\" response (denoted in <FCR></FCR>), otherwise return a plain text response.\n",
      "\n",
      "  ### Input:\n",
      "  sumarize this in 1 sentence https://openai.com/blog/function-calling-and-other-api-updates\n",
      "  <FC>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FC>\n",
      "  ### Response:\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "  <FCR>function call (sumarize this in 1 sentence https://openai.com/blog/function-calling-and-other-api-updates)</FCR>\n",
      "\n",
      "\n",
      "  ### Input:\n",
      "  <FC>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"\n",
      "Ground truth:\n",
      "  <FCR>{\"arguments\": \"{\\\"json\\\": {\\\"link\\\": \\\"https://openai.com/blog/function-calling-and-other-api-updates\\\"}}\", \"name\": \"transcodeWebPage\"}</FCR>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "sample = dataset[0][\"text\"]\n",
    "\n",
    "inp, target = sample.split(\"### Response:\\n\")\n",
    "prompt = inp + \"### Response:\\n\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{target}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
