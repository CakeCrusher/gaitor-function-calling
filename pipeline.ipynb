{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e1455-6621-4198-a135-886bf1950aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors>=0.3.1\" ipywidgets wandb --upgrade\n",
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=1 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afb05ca-9182-4431-b115-0dcae7ddee89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(project_dir)\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25034482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:llama-7-chat-instruction-int4-fc-op_glaive-sft) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-7-chat-instruction-int4-fc-op_glaive-sft</strong> at: <a href='https://wandb.ai/sebastiansosa/function_calling/runs/llama-7-chat-instruction-int4-fc-op_glaive-sft' target=\"_blank\">https://wandb.ai/sebastiansosa/function_calling/runs/llama-7-chat-instruction-int4-fc-op_glaive-sft</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231204_155638-llama-7-chat-instruction-int4-fc-op_glaive-sft/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:llama-7-chat-instruction-int4-fc-op_glaive-sft). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7380f4287a6d4c7185f83bdd00efaf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111187191369633, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sosa.s/gaitor_function_calling/wandb/run-20231204_155800-llama-7-chat-instruction-int4-fc-op_glaive-sft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sebastiansosa/function_calling/runs/llama-7-chat-instruction-int4-fc-op_glaive-sft' target=\"_blank\">llama-7-chat-instruction-int4-fc-op_glaive-sft</a></strong> to <a href='https://wandb.ai/sebastiansosa/function_calling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sebastiansosa/function_calling' target=\"_blank\">https://wandb.ai/sebastiansosa/function_calling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sebastiansosa/function_calling/runs/llama-7-chat-instruction-int4-fc-op_glaive-sft' target=\"_blank\">https://wandb.ai/sebastiansosa/function_calling/runs/llama-7-chat-instruction-int4-fc-op_glaive-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450b806df83b4d9798a239481d5e25c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configs\n",
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "output_dir = \"llama-7-chat-instruction-int4-fc-op_glaive-sft\"\n",
    "hub_model_id = f\"SebastianS/{output_dir}\"\n",
    "wandb.init(project=\"function_calling\", id=output_dir, entity=\"sebastiansosa\", notes=\"Trained for 3 epochs on shuffled data from openplugin and glaive datasets with a train test split of 99.7% train.\")\n",
    "wandb.define_metric(\"fc_combine\", goal=\"maximize\")\n",
    "wandb.define_metric(\"exact_match\", goal=\"maximize\")\n",
    "wandb.define_metric(\"perplexity\", goal=\"minimize\")\n",
    "wandb.define_metric(\"epoch\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e4cd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9820\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data and convert to Datasets format\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from gaitor_function_calling.data.data_utils import DataAbstractor\n",
    "from gaitor_function_calling.data.prompting_utils import INSTRUCTION\n",
    "\n",
    "data_abstractor = DataAbstractor(\"op_glaive_10k.json\", \"full_sft\")\n",
    "# train, test = data_abstractor.build_data(INSTRUCTION, True, .997)\n",
    "# data_abstractor.save(data_abstractor.raw_data, train, test)\n",
    "\n",
    "train_df = pd.DataFrame(data_abstractor.train_data)\n",
    "test_df = pd.DataFrame(data_abstractor.test_data)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf42525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup metrics and evaluation\n",
    "import json\n",
    "from transformers import EvalPrediction\n",
    "from gaitor_function_calling.evaluation.evaluation_utils import FunctionCallingMetric, compute_perplexity, get_logits_and_labels\n",
    "from gaitor_function_calling.data.prompting_utils import INSTRUCTION, json_arguments_from_prompt, generate_prediction\n",
    "import numpy as np\n",
    "\n",
    "fc_metric = FunctionCallingMetric()\n",
    "\n",
    "def custom_evaluation(eval_dataset, model, tokenizer, epoch, save_prompts_path=False):\n",
    "    print(\"Starting custom evaluation.\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    results = {}\n",
    "\n",
    "    fc_results = []\n",
    "    exact_match_results = []\n",
    "    perplexity_results = []\n",
    "\n",
    "\n",
    "    if save_prompts_path:\n",
    "        try:\n",
    "            with open(save_prompts_path, \"r\") as f:\n",
    "                dpo_data = json.load(f)\n",
    "        except:\n",
    "            dpo_data = []\n",
    "\n",
    "\n",
    "    for idx, example in enumerate(eval_dataset):\n",
    "        print(f\"Example {idx}: \", end=\"\")\n",
    "        post_message = \"\"\n",
    "        \n",
    "        # Custom Function Calling metric\n",
    "        prompts = None\n",
    "        try:\n",
    "            generated_str = generate_prediction(example[\"text\"], model, tokenizer, INSTRUCTION)\n",
    "            generated_arguments, expected_arguments, prompts = json_arguments_from_prompt(\n",
    "                example[\"text\"],\n",
    "                generated_str,\n",
    "                INSTRUCTION,\n",
    "                {\"idx\": idx, \"epoch\": epoch}\n",
    "            )\n",
    "            fc_result = fc_metric.run(generated_arguments, expected_arguments)\n",
    "\n",
    "            fc_results.append(fc_result)\n",
    "\n",
    "            if save_prompts_path:\n",
    "                # add prompts to dpo_data.json\n",
    "                dpo_data.append({\n",
    "                    \"fc_result\": fc_result,\n",
    "                    **prompts\n",
    "                })\n",
    "                with open(save_prompts_path, \"w\") as f:\n",
    "                    json.dump(dpo_data, f)\n",
    "        except Exception as e:\n",
    "            post_message += f\"Error function calling: {e}\\n\"\n",
    "            fc_results.append(0)\n",
    "\n",
    "        # exact match metric\n",
    "        if prompts:\n",
    "            exact_match_res = fc_metric._sentence_similarity(prompts[\"expected_str\"].split(\"[/INST]\")[1], prompts[\"generated_str\"].split(\"[/INST]\")[1])\n",
    "            exact_match_results.append(exact_match_res)\n",
    "        else:\n",
    "            exact_match_results.append(0)\n",
    "\n",
    "        # perplexity metric\n",
    "        try:\n",
    "            logits, labels = get_logits_and_labels(example[\"text\"], model, tokenizer)\n",
    "            perplexity = compute_perplexity(logits[..., :-1, :], labels).item()\n",
    "            perplexity_results.append(perplexity)\n",
    "        except Exception as e:\n",
    "            post_message += f\"Error perplexity: {e}\\n\"\n",
    "            # perplexity_results.append(float('inf'))\n",
    "\n",
    "        example_metric = {\n",
    "            \"fc_combine\": fc_results[-1],\n",
    "            \"exact_match\": exact_match_results[-1],\n",
    "            \"perplexity\": perplexity_results[-1] if len(perplexity_results) else None\n",
    "        }\n",
    "        print(example_metric)\n",
    "        if post_message:\n",
    "            print(post_message)\n",
    "        \n",
    "    results[\"fc_combine\"] =  sum(fc_results) / len(fc_results)\n",
    "    results[\"exact_match\"] =  sum(exact_match_results) / len(exact_match_results)\n",
    "    results[\"perplexity\"] =  sum(perplexity_results) / len(perplexity_results) if len(perplexity_results) else 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece0fe40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf29cfa4c114d12a271540d85aa7080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base LLM model and tokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2b59eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# peft config on the model\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f62443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training config\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=3,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,  # Enable pushing to Hub\n",
    "    hub_model_id=hub_model_id,  # Hugging Face Hub model ID\n",
    "    hub_strategy=\"every_save\",  # Push to Hub every epoch\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    # eval_dataset=dataset_dict[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83bbf4f2-26dd-4a98-83b0-2928e24f148e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "others = {\n",
    "            \"fc_combine\": ,\n",
    "            \"exact_match\": exact_match_results[-1],\n",
    "            \"perplexity\": perplexity_results[-1] if len(perplexity_results) else None\n",
    "        }\n",
    "wandb.log({\"epoch\": epoch,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77897708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting custom evaluation.\n",
      "Example 0: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.66923189163208}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 1: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 6.339871883392334}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 2: {'fc_combine': 1.0, 'exact_match': 0.7547522187232971, 'perplexity': 9.583250999450684}\n",
      "Example 3: {'fc_combine': 1.0, 'exact_match': 0.8241101503372192, 'perplexity': 7.283013820648193}\n",
      "Example 4: {'fc_combine': 1.0, 'exact_match': 0.8586133122444153, 'perplexity': 8.35544490814209}\n",
      "Example 5: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 8.844438552856445}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 6: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 6.977672100067139}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 7: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 5.191576957702637}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 8: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.383997440338135}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 9: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 5.647104740142822}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 10: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.954756259918213}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 11: {'fc_combine': 0.9457772970199585, 'exact_match': 0.6955670118331909, 'perplexity': 10.47035026550293}\n",
      "Example 12: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 8.468426704406738}\n",
      "Error function calling: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Example 13: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.743885517120361}\n",
      "Error function calling: 'NoneType' object has no attribute 'group'\n",
      "\n",
      "Example 14: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.580165863037109}\n",
      "Error function calling: Extra data: line 1 column 3 (char 2)\n",
      "\n",
      "Example 15: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.4108805656433105}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 16: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 10.88608169555664}\n",
      "Error function calling: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Example 17: {'fc_combine': 0.8397398069500923, 'exact_match': 0.8202518820762634, 'perplexity': 7.851646900177002}\n",
      "Example 18: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.2042131423950195}\n",
      "Error function calling: 'NoneType' object has no attribute 'group'\n",
      "\n",
      "Example 19: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 8.470029830932617}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 20: {'fc_combine': 0.6974997917811075, 'exact_match': 0.6177162528038025, 'perplexity': 6.465075492858887}\n",
      "Example 21: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.117079734802246}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 22: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 6.089100360870361}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 23: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 9.498514175415039}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 24: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 8.272503852844238}\n",
      "Error function calling: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Example 25: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 6.340742588043213}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 26: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.760767936706543}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Example 27: {'fc_combine': 1.0, 'exact_match': 0.8124143481254578, 'perplexity': 7.508194446563721}\n",
      "Example 28: {'fc_combine': 0.7075552567839622, 'exact_match': 0.7770057916641235, 'perplexity': 8.819174766540527}\n",
      "Example 29: {'fc_combine': 0, 'exact_match': 0, 'perplexity': 7.755074977874756}\n",
      "Error function calling: No function call found in generated data\n",
      "\n",
      "Evaluation results for epoch -1: {'fc_combine': 0.23968573841783736, 'exact_match': 0.20534769892692567, 'perplexity': 7.764742279052735}\n",
      "Begin epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5622, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.8084, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.4028, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.2104, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.2042, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 0.1583, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.1856, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.1575, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.1854, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.1256, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.1418, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.1351, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.1156, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.1169, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.1811, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.156, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.1086, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.0897, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.1016, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.1086, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.1004, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.1287, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.1154, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 0.0834, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0821, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.1014, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.092, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.1546, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.0954, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0811, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0794, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 0.114, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.1141, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.1206, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0713, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0914, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 0.0962, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.1366, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.1042, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.1199, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.1128, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.1184, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.1266, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.074, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0949, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0671, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 0.1154, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.1031, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.0817, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 0.0628, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0789, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0784, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.1, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.089, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.0816, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0975, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.1283, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.0751, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.11, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.0852, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.1075, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.084, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.086, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0575, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.0727, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0874, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0767, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 0.0709, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.0898, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.0912, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0729, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0714, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 0.0743, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0643, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.1019, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.076, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.0901, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 0.0801, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.1055, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.075, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.1155, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.0933, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.1126, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0715, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0911, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 0.0929, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.0804, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.0913, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0992, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0852, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 0.0673, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.07, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.073, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.0569, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.0632, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 0.0726, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0691, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0852, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.0797, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.0846, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0755, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0872, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0736, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 0.1021, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0605, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0709, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.095, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.0629, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 0.073, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0618, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0698, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0609, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0999, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 0.0687, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0817, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0925, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0784, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0673, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 0.0684, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.1068, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.057, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 0.0585, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0738, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0543, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.0778, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.0992, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 0.0687, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0622, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0737, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0837, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0809, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 0.0783, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.0641, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.1376, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0581, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0731, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 0.0826, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.0884, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.0708, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 0.0893, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0639, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0725, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.0571, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.1066, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 0.0872, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0616, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0672, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0721, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0576, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 0.0651, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.055, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0883, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0698, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0639, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 0.1273, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0856, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.064, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.074, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0731, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0842, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.093, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.0669, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 0.0637, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2478.6361, 'train_samples_per_second': 3.962, 'train_steps_per_second': 0.66, 'train_loss': 0.10604983327879583, 'epoch': 1.0}\n",
      "Starting custom evaluation.\n",
      "Example 0: {'fc_combine': 1.0, 'exact_match': 0.9981851577758789, 'perplexity': 1.087579369544983}\n",
      "Example 1: {'fc_combine': 1.0, 'exact_match': 0.6529404520988464, 'perplexity': 1.0964655876159668}\n",
      "Example 2: {'fc_combine': 1.0, 'exact_match': 0.9980251789093018, 'perplexity': 1.083078145980835}\n",
      "Example 3: {'fc_combine': 1.0, 'exact_match': 0.9787054657936096, 'perplexity': 1.1633362770080566}\n",
      "Example 4: {'fc_combine': 1.0, 'exact_match': 0.6212353706359863, 'perplexity': 1.0716067552566528}\n",
      "Example 5: {'fc_combine': 0.909208357334137, 'exact_match': 0.995028018951416, 'perplexity': 1.1282819509506226}\n",
      "Example 6: {'fc_combine': 1.0, 'exact_match': 0.9976781010627747, 'perplexity': 1.0738250017166138}\n",
      "Example 7: {'fc_combine': 1.0, 'exact_match': 0.9980186223983765, 'perplexity': 1.129141926765442}\n",
      "Example 8: {'fc_combine': 1.0, 'exact_match': 0.9982511401176453, 'perplexity': 1.1299036741256714}\n",
      "Example 9: {'fc_combine': 1.0, 'exact_match': 0.9984581470489502, 'perplexity': 1.0514822006225586}\n",
      "Example 10: {'fc_combine': 1.0, 'exact_match': 0.7834185361862183, 'perplexity': 1.0727635622024536}\n",
      "Example 11: {'fc_combine': 1.0, 'exact_match': 0.9982395768165588, 'perplexity': 1.1245899200439453}\n",
      "Example 12: {'fc_combine': 0.8144461587071419, 'exact_match': 0.6510248780250549, 'perplexity': 1.1346755027770996}\n",
      "Example 13: {'fc_combine': 0.75, 'exact_match': 0.998069703578949, 'perplexity': 1.095314383506775}\n",
      "Example 14: {'fc_combine': 1.0, 'exact_match': 0.6597790122032166, 'perplexity': 1.0657086372375488}\n",
      "Example 15: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.064570665359497}\n",
      "Example 16: {'fc_combine': 0.8780942360560099, 'exact_match': 0.9047735333442688, 'perplexity': 1.1372522115707397}\n",
      "Example 17: {'fc_combine': 0.7312084445729852, 'exact_match': 0.8807039260864258, 'perplexity': 1.1145179271697998}\n",
      "Example 18: {'fc_combine': 1.0, 'exact_match': 0.9970979690551758, 'perplexity': 1.1313005685806274}\n",
      "Example 19: {'fc_combine': 1.0, 'exact_match': 0.7362882494926453, 'perplexity': 1.2258012294769287}\n",
      "Example 20: {'fc_combine': 0.8910068372885386, 'exact_match': 0.9502963423728943, 'perplexity': 1.132575511932373}\n",
      "Example 21: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.0699269771575928}\n",
      "Example 22: {'fc_combine': 1.0, 'exact_match': 0.9986801147460938, 'perplexity': 1.1431314945220947}\n",
      "Example 23: {'fc_combine': 0.8099867403507233, 'exact_match': 0.9829595685005188, 'perplexity': 1.1778693199157715}\n",
      "Example 24: {'fc_combine': 1.0, 'exact_match': 0.9981951117515564, 'perplexity': 1.1349073648452759}\n",
      "Example 25: {'fc_combine': 1.0, 'exact_match': 0.9984179139137268, 'perplexity': 1.0856698751449585}\n",
      "Example 26: {'fc_combine': 1.0, 'exact_match': 0.9981851577758789, 'perplexity': 1.074884057044983}\n",
      "Example 27: {'fc_combine': 1.0, 'exact_match': 0.9981421828269958, 'perplexity': 1.0610071420669556}\n",
      "Example 28: {'fc_combine': 1.0, 'exact_match': 0.9988278746604919, 'perplexity': 1.0642865896224976}\n",
      "Example 29: {'fc_combine': 1.0, 'exact_match': 0.9982583522796631, 'perplexity': 1.0736042261123657}\n",
      "Evaluation results for epoch 0: {'fc_combine': 0.9594650258103179, 'exact_match': 0.9254744410514831, 'perplexity': 1.1066352685292562}\n",
      "\n",
      "\n",
      "\n",
      "Begin epoch 1\n",
      "{'loss': 0.0608, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.0609, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.0541, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.0534, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.078, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 0.0876, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.0629, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.0508, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.0938, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.0803, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.0659, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.1036, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.069, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0553, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0505, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0805, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.0504, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.0613, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.0704, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.0623, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.0656, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.0597, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.0483, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 0.0467, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0789, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0509, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.0595, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.0514, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.0537, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0714, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0526, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 0.0626, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.0853, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.0595, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.068, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0564, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 0.0689, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.0625, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.0728, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.0861, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.067, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.0739, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.0666, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.0649, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0493, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0457, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 0.0715, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.0759, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.0752, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 0.056, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0623, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0621, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.0636, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.05, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.0587, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0975, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0613, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.0993, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.0598, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.0682, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.0513, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.0653, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0532, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0498, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.0505, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0598, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0678, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 0.1024, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.1145, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.0562, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0478, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0562, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 0.0459, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0496, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0497, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.071, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.0684, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 0.0584, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.0594, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.0461, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.0629, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.049, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.058, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0484, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0705, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 0.0673, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.066, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.0493, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0474, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.057, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 0.0607, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.056, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.0601, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.0708, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.0559, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 0.0502, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0444, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0533, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.052, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.0634, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0651, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.063, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0511, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 0.1023, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0623, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0527, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.0781, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.0562, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 0.0678, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0757, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0751, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0505, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0631, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 0.0645, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0537, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0629, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0472, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0541, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 0.0514, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.0615, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.0536, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 0.0607, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0778, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0677, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.059, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.0495, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 0.0663, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0959, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0556, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0517, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0583, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 0.0687, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.056, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.0551, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0525, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0719, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 0.0459, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.05, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.04, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 0.0656, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0525, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0681, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.0503, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.0472, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 0.0784, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0561, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0434, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0564, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0501, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 0.0698, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.051, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0626, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0693, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0465, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 0.0667, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0509, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0528, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0558, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0567, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0789, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.0476, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.0527, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 0.0592, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2482.3916, 'train_samples_per_second': 3.956, 'train_steps_per_second': 0.659, 'train_loss': 0.06186834081470347, 'epoch': 1.0}\n",
      "Starting custom evaluation.\n",
      "Example 0: {'fc_combine': 1.0, 'exact_match': 0.9981851577758789, 'perplexity': 1.0963603258132935}\n",
      "Example 1: {'fc_combine': 1.0, 'exact_match': 0.9985989928245544, 'perplexity': 1.0946937799453735}\n",
      "Example 2: {'fc_combine': 1.0, 'exact_match': 0.9980251789093018, 'perplexity': 1.0904738903045654}\n",
      "Example 3: {'fc_combine': 1.0, 'exact_match': 0.9982576370239258, 'perplexity': 1.1819417476654053}\n",
      "Example 4: {'fc_combine': 1.0, 'exact_match': 0.9975748658180237, 'perplexity': 1.0779368877410889}\n",
      "Example 5: {'fc_combine': 0.9088090062141418, 'exact_match': 0.9946353435516357, 'perplexity': 1.1240414381027222}\n",
      "Example 6: {'fc_combine': 0.8333333333333334, 'exact_match': 0.9968423247337341, 'perplexity': 1.0795047283172607}\n",
      "Example 7: {'fc_combine': 1.0, 'exact_match': 0.9980186223983765, 'perplexity': 1.141769528388977}\n",
      "Example 8: {'fc_combine': 1.0, 'exact_match': 0.9982511401176453, 'perplexity': 1.1396771669387817}\n",
      "Example 9: {'fc_combine': 1.0, 'exact_match': 0.9984581470489502, 'perplexity': 1.040805697441101}\n",
      "Example 10: {'fc_combine': 1.0, 'exact_match': 0.6873627305030823, 'perplexity': 1.0842180252075195}\n",
      "Example 11: {'fc_combine': 1.0, 'exact_match': 0.714484691619873, 'perplexity': 1.1133042573928833}\n",
      "Example 12: {'fc_combine': 0.8144461587071419, 'exact_match': 0.9824089407920837, 'perplexity': 1.1424654722213745}\n",
      "Example 13: {'fc_combine': 0.75, 'exact_match': 0.998069703578949, 'perplexity': 1.0988562107086182}\n",
      "Example 14: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.0722522735595703}\n",
      "Example 15: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.0669161081314087}\n",
      "Example 16: {'fc_combine': 0.9076203107833862, 'exact_match': 0.9391551613807678, 'perplexity': 1.1487451791763306}\n",
      "Example 17: {'fc_combine': 1.0, 'exact_match': 0.9982011914253235, 'perplexity': 1.1117615699768066}\n",
      "Example 18: {'fc_combine': 1.0, 'exact_match': 0.9970979690551758, 'perplexity': 1.1060922145843506}\n",
      "Example 19: {'fc_combine': 1.0, 'exact_match': 0.9975748658180237, 'perplexity': 1.2483267784118652}\n",
      "Example 20: {'fc_combine': 0.9337893923123678, 'exact_match': 0.9923182725906372, 'perplexity': 1.1356614828109741}\n",
      "Example 21: {'fc_combine': 1.0, 'exact_match': 0.7821587324142456, 'perplexity': 1.0742595195770264}\n",
      "Example 22: {'fc_combine': 0.9637727439403534, 'exact_match': 0.9962430000305176, 'perplexity': 1.1372753381729126}\n",
      "Example 23: {'fc_combine': 0.7527457773685455, 'exact_match': 0.9322625994682312, 'perplexity': 1.164074182510376}\n",
      "Example 24: {'fc_combine': 1.0, 'exact_match': 0.9981951117515564, 'perplexity': 1.1337617635726929}\n",
      "Example 25: {'fc_combine': 1.0, 'exact_match': 0.9984179139137268, 'perplexity': 1.0881707668304443}\n",
      "Example 26: {'fc_combine': 1.0, 'exact_match': 0.9981851577758789, 'perplexity': 1.086808204650879}\n",
      "Example 27: {'fc_combine': 1.0, 'exact_match': 0.9981421828269958, 'perplexity': 1.0787049531936646}\n",
      "Example 28: {'fc_combine': 0.8014821782708168, 'exact_match': 0.9519480466842651, 'perplexity': 1.062941074371338}\n",
      "Example 29: {'fc_combine': 1.0, 'exact_match': 0.9982583522796631, 'perplexity': 1.0746173858642578}\n",
      "Evaluation results for epoch 1: {'fc_combine': 0.9555332966976695, 'exact_match': 0.96445605357488, 'perplexity': 1.1098805983861288}\n",
      "\n",
      "\n",
      "\n",
      "Begin epoch 2\n",
      "{'loss': 0.0504, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.0631, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 0.069, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.0539, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.0466, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 0.0558, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.0496, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.0545, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.0489, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.051, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.0506, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0614, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.055, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.0508, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0533, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0544, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.0527, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 0.0493, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.0697, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.0422, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.0554, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.0494, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.0684, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 0.0529, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0443, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0495, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.0745, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 0.044, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.0474, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0501, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.0509, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 0.0505, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.0514, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
      "{'loss': 0.0538, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0565, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0481, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 0.0506, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.0496, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 0.054, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.0777, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.0565, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.0477, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.0466, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.0514, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0438, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0489, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 0.061, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.0463, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.0781, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 0.0472, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0445, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "{'loss': 0.0487, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.059, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.0581, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.0849, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0576, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0596, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.0622, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 0.0463, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.0641, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.0796, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.061, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0537, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0512, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.0445, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0511, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.0484, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 0.0601, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.0506, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.0504, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0452, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0498, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 0.0426, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0493, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0435, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.0533, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.042, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 0.0466, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.0556, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
      "{'loss': 0.0484, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.0636, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.0446, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.0536, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0505, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0589, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 0.0453, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.0407, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.0456, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0423, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0533, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
      "{'loss': 0.076, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.0624, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.0451, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.052, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.0596, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 0.0657, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0486, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0416, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.0475, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.093, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0551, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0498, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0516, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 0.0472, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.052, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0431, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.0488, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.044, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 0.0654, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0471, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0613, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0488, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0461, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 0.0507, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0502, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 0.0518, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0417, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0833, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
      "{'loss': 0.0443, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.0443, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.049, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 0.0469, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0508, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0459, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.0429, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 0.0477, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 0.0566, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0637, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
      "{'loss': 0.0438, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.07, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0579, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 0.0429, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.0464, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.0464, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0533, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0562, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 0.042, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.0445, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.0612, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 0.0636, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0612, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0436, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.042, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
      "{'loss': 0.0569, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 0.0448, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0622, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0432, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0548, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0499, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
      "{'loss': 0.0572, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0556, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0548, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.057, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0492, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "{'loss': 0.0544, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0466, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0541, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0469, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0398, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0539, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.0552, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.0448, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 0.0555, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2482.2198, 'train_samples_per_second': 3.956, 'train_steps_per_second': 0.659, 'train_loss': 0.05296873995350422, 'epoch': 1.0}\n",
      "Starting custom evaluation.\n",
      "Example 0: {'fc_combine': 1.0, 'exact_match': 0.9981851577758789, 'perplexity': 1.0992646217346191}\n",
      "Example 1: {'fc_combine': 1.0, 'exact_match': 0.9985989928245544, 'perplexity': 1.1022732257843018}\n",
      "Example 2: {'fc_combine': 1.0, 'exact_match': 0.9980251789093018, 'perplexity': 1.1027696132659912}\n",
      "Example 3: {'fc_combine': 1.0, 'exact_match': 0.9982576370239258, 'perplexity': 1.1957300901412964}\n",
      "Example 4: {'fc_combine': 1.0, 'exact_match': 0.9975748658180237, 'perplexity': 1.0843491554260254}\n",
      "Example 5: {'fc_combine': 1.0, 'exact_match': 0.9975206851959229, 'perplexity': 1.1199218034744263}\n",
      "Example 6: {'fc_combine': 1.0, 'exact_match': 0.9976781010627747, 'perplexity': 1.0864551067352295}\n",
      "Example 7: {'fc_combine': 1.0, 'exact_match': 0.9980186223983765, 'perplexity': 1.1364262104034424}\n",
      "Example 8: {'fc_combine': 1.0, 'exact_match': 0.9982511401176453, 'perplexity': 1.1362069845199585}\n",
      "Example 9: {'fc_combine': 0.7677328586578369, 'exact_match': 0.9910032153129578, 'perplexity': 1.0373289585113525}\n",
      "Example 10: {'fc_combine': 1.0, 'exact_match': 0.9975748658180237, 'perplexity': 1.0838892459869385}\n",
      "Example 11: {'fc_combine': 1.0, 'exact_match': 0.9982395768165588, 'perplexity': 1.1258481740951538}\n",
      "Example 12: {'fc_combine': 0.8144461587071419, 'exact_match': 0.9824089407920837, 'perplexity': 1.1452206373214722}\n",
      "Example 13: {'fc_combine': 0.75, 'exact_match': 0.998069703578949, 'perplexity': 1.1081088781356812}\n",
      "Example 14: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.084829330444336}\n",
      "Example 15: {'fc_combine': 1.0, 'exact_match': 0.998174786567688, 'perplexity': 1.08688485622406}\n",
      "Example 16: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model with the custom evaluation logic\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_result})\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the model after each epoch (optional)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mcustom_evaluation\u001b[0;34m(eval_dataset, model, tokenizer, epoch, save_prompts_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     generated_str \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINSTRUCTION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     generated_arguments, expected_arguments, prompts \u001b[38;5;241m=\u001b[39m json_arguments_from_prompt(\n\u001b[1;32m     37\u001b[0m         example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m         generated_str,\n\u001b[1;32m     39\u001b[0m         INSTRUCTION,\n\u001b[1;32m     40\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m: idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch}\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     fc_result \u001b[38;5;241m=\u001b[39m fc_metric\u001b[38;5;241m.\u001b[39mrun(generated_arguments, expected_arguments)\n",
      "File \u001b[0;32m~/gaitor_function_calling/data/prompting_utils.py:97\u001b[0m, in \u001b[0;36mgenerate_prediction\u001b[0;34m(data_text, model, tokenizer, instruction)\u001b[0m\n\u001b[1;32m     95\u001b[0m prompt \u001b[38;5;241m=\u001b[39m inp \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 97\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m prediction \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:635\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:351\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    349\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    350\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 351\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    354\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora.py:1123\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_adapters \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:574\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemv_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/functional.py:1519\u001b[0m, in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcgemm_4bit_inference_naive_bf16(m, n, k, get_ptr(A), get_ptr(B), get_ptr(absmax), get_ptr(state[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), get_ptr(out), lda, ldb, ldc, ct\u001b[38;5;241m.\u001b[39mc_int32(state[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcgemm_4bit_inference_naive_fp32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatmul not implemented for data type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training and Evaluation Loop\n",
    "save_eval_path = \"./data/prompts/\"+os.path.basename(data_abstractor.paths[\"train_dir\"]+\".json\")\n",
    "eval_result = custom_evaluation(dataset_dict[\"test\"], model, tokenizer, -1, save_eval_path)\n",
    "print(f\"Evaluation results for epoch {-1}: {eval_result}\")\n",
    "wandb.log({\"epoch\": -1, **eval_result})\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Begin epoch {epoch}\")\n",
    "    # Train for one epoch\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model with the custom evaluation logic\n",
    "    eval_result = custom_evaluation(dataset_dict[\"test\"], model, tokenizer, epoch, save_eval_path)\n",
    "\n",
    "    wandb.log({\"epoch\": epoch, **eval_result})\n",
    "\n",
    "    # Save the model after each epoch (optional)\n",
    "    trainer.save_model()\n",
    "\n",
    "    \n",
    "\n",
    "    # Log or print evaluation results\n",
    "    print(f\"Evaluation results for epoch {epoch}: {eval_result}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9637ba-fcc3-4368-9998-9dbb92da0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f99660-d438-4884-b873-8a193c2e2c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
