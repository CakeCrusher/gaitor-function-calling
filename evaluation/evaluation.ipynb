{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/sosa.s/.cache/huggingface/datasets/json/default-029541ac98941321/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "<FUNCTIONS>[{\"name\": \"transcodeWebPage\", \"description\": \"Acquire precise webpage details or real-time search engine responses based on user-input content.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"link\": {\"type\": \"string\", \"description\": \"This parameter takes either a URL or a non-URL string. If a URL is given, the model will engage with the designated webpage to collect or interact with its data. If a non-URL string is given, the model will handle it as a search inquiry and try to find related real-time news or information. To guarantee the best results, make sure the input is a valid URL or a succinct search query.\"}}, \"type\": \"object\"}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "Just checking [/INST] <FUNCTION_CALL_NAME>getSearchNews</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"params\": {\"country\": \"us\", \"language\": \"en\", \"topic\": \"Bath and Body Works\"}}</FUNCTION_CALL_ARGUMENTS> </s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy\n",
    "relative_path_to_data = './production_eval_chat.json'\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "print(dataset[30][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f71ba0529c049d5bbafe50c3d550e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "hub_id = \"SebastianS/function_calling-llama_7b\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "fc_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    hub_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "fc_tokenizer = AutoTokenizer.from_pretrained(hub_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "embedding_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "function_calling_tokens = {\n",
    "    \"FUNCTIONS\": {\n",
    "        \"start\": \"<FUNCTIONS>\",\n",
    "        \"end\": \"</FUNCTIONS>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_NAME\": {\n",
    "        \"start\": \"<FUNCTION_CALL_NAME>\",\n",
    "        \"end\": \"</FUNCTION_CALL_NAME>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_ARGUMENTS\": {\n",
    "        \"start\": \"<FUNCTION_CALL_ARGUMENTS>\",\n",
    "        \"end\": \"</FUNCTION_CALL_ARGUMENTS>\"\n",
    "    },\n",
    "    \"all\": [\"<FUNCTIONS>\", \"</FUNCTIONS>\", \"<FUNCTION_CALL_NAME>\", \"</FUNCTION_CALL_NAME>\", \"<FUNCTION_CALL_ARGUMENTS>\", \"</FUNCTION_CALL_ARGUMENTS>\"]\n",
    "}\n",
    "\n",
    "def parse_prompt_back_to_data(prompt):\n",
    "    \"\"\"\n",
    "    Function to parse a prompt back into the original data format, using a dictionary of function calling tokens.\n",
    "    \n",
    "    :param prompt: A string representing the constructed prompt.\n",
    "    :param function_calling_tokens: A dictionary containing the start and end tokens for different function call elements.\n",
    "    :return: A dictionary representing the original data instance.\n",
    "    \"\"\"\n",
    "    # Building regular expression patterns using the function_calling_tokens\n",
    "    functions_pattern = rf\"{function_calling_tokens['FUNCTIONS']['start']}(.*?){function_calling_tokens['FUNCTIONS']['end']}\"\n",
    "    input_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"  # This remains unchanged as it's not part of function_calling_tokens\n",
    "    target_content_pattern = r\"\\[/INST\\] (.*)</s>\"  # This also remains unchanged\n",
    "    function_call_name_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_NAME']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_NAME']['end']}\"\n",
    "    function_call_arguments_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}\"\n",
    "\n",
    "    # Extracting data using regular expressions\n",
    "    functions_str = re.search(functions_pattern, prompt).group(1)\n",
    "    input_content = re.search(input_pattern, prompt).group(1)\n",
    "    target_content_match = re.search(target_content_pattern, prompt)\n",
    "\n",
    "    # Parse functions JSON string\n",
    "    functions = json.loads(functions_str)\n",
    "\n",
    "    # Prepare the data dictionary\n",
    "    data = {\n",
    "        \"input\": [{\n",
    "            \"chatgptMessage\": {\"role\": \"user\", \"content\": input_content},\n",
    "            \"functions\": functions\n",
    "        }],\n",
    "        \"target\": {\n",
    "            \"chatgptMessage\": {\"role\": \"assistant\"},\n",
    "            \"functions\": functions  # Including functions in the target as well\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the target has a function call\n",
    "    if function_calling_tokens['FUNCTION_CALL_NAME']['start'] in prompt:\n",
    "        function_call_name = re.search(function_call_name_pattern, prompt).group(1)\n",
    "        function_call_arguments = re.search(function_call_arguments_pattern, prompt).group(1)\n",
    "        data[\"target\"][\"chatgptMessage\"][\"function_call\"] = {\n",
    "            \"name\": function_call_name,\n",
    "            \"arguments\": function_call_arguments\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where regex might not find a match for target content\n",
    "        if target_content_match:\n",
    "            target_content = target_content_match.group(1)\n",
    "            data[\"target\"][\"chatgptMessage\"][\"content\"] = target_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embedding_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "\n",
    "    return mean_pooled[0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    def compare_json(g_json, e_json, key_similarity_scores, value_similarity_scores):\n",
    "        for e_key, e_value in e_json.items():\n",
    "            # Check for exact key match or find the most similar key\n",
    "            if e_key in g_json:\n",
    "                g_key = e_key\n",
    "                key_similarity_scores.append(1)\n",
    "            else:\n",
    "                # Compute similarity with all keys in generated_json and find the best match\n",
    "                key_similarity = {gen_key: sentence_similarity(e_key, gen_key) for gen_key in g_json.keys()}\n",
    "                g_key, key_sim_score = max(key_similarity.items(), key=lambda x: x[1])\n",
    "                key_similarity_scores.append(key_sim_score)\n",
    "\n",
    "            # Recursive comparison for nested objects, else compare values\n",
    "            if isinstance(e_value, dict) and isinstance(g_json.get(g_key, {}), dict):\n",
    "                compare_json(g_json[g_key], e_value, key_similarity_scores, value_similarity_scores)\n",
    "            elif isinstance(e_value, str) and isinstance(g_json.get(g_key, \"\"), str):\n",
    "                # Compare values only if they are strings at the root level\n",
    "                value_sim_score = sentence_similarity(e_value, g_json[g_key])\n",
    "                value_similarity_scores.append(value_sim_score)\n",
    "            elif e_value == g_json.get(g_key, None):\n",
    "                value_similarity_scores.append(1)  # Exact match for non-string root values\n",
    "            else:\n",
    "                value_similarity_scores.append(0)  # Non-matching root values\n",
    "\n",
    "    key_similarity_scores = []\n",
    "    value_similarity_scores = []\n",
    "    compare_json(generated_json, expected_json, key_similarity_scores, value_similarity_scores)\n",
    "\n",
    "    # Compute the average similarity scores\n",
    "    avg_key_similarity = sum(key_similarity_scores) / len(key_similarity_scores) if key_similarity_scores else 0\n",
    "    avg_value_similarity = sum(value_similarity_scores) / len(value_similarity_scores) if value_similarity_scores else 0\n",
    "\n",
    "    return (avg_key_similarity + avg_value_similarity) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Metric score: 0\n",
      "2 Metric score: 0\n",
      "3 Metric score: 0\n",
      "4 Metric score: 0\n",
      "5 Metric score: 0\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "7 Metric score: 0\n",
      "8 Metric score: 0\n",
      "9 Metric score: 0\n",
      "10 Metric score: 0\n",
      "11 Metric score: 0\n",
      "12 Metric score: 0\n",
      "13 Metric score: 0\n",
      "14 Metric score: 0\n",
      "15 Metric score: 0\n",
      "16 Metric score: 0\n",
      "17 Metric score: 0\n",
      "18 Metric score: 0\n",
      "19 Metric score: 0\n",
      "20 Metric score: 0\n",
      "21 Metric score: 0\n",
      "22 Metric score: 0\n",
      "23 Metric score: 0\n",
      "24 Metric score: 0\n",
      "25 Metric score: 0\n",
      "26 Metric score: 0\n",
      "27 Metric score: 0\n",
      "28 Metric score: 0\n",
      "29 Metric score: 0\n",
      "30 Metric score: 0\n",
      "31 Metric score: 0\n",
      "32 Metric score: 0\n",
      "33 Metric score: 0\n",
      "34 Metric score: 0\n",
      "35 Metric score: 0\n",
      "36 Metric score: 0\n",
      "37 Metric score: 0\n",
      "38 Metric score: 0\n",
      "39 Metric score: 0\n",
      "40 Metric score: 0\n",
      "41 Metric score: 0\n",
      "42 Metric score: 0\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "45 Metric score: 0\n",
      "46 Metric score: 0\n",
      "47 Metric score: 0\n",
      "48 Metric score: 0\n",
      "49 Metric score: 0\n",
      "50 Metric score: 0\n",
      "51 Metric score: 0\n",
      "52 Metric score: 0\n",
      "53 Metric score: 0\n",
      "54 Metric score: 0\n",
      "55 Metric score: 0\n",
      "56 Metric score: 0\n",
      "57 Metric score: 0\n",
      "58 Metric score: 0\n",
      "59 Metric score: 0\n",
      "60 Metric score: 0\n",
      "61 Metric score: 0\n",
      "62 Metric score: 0\n",
      "63 Metric score: 0\n",
      "64 Metric score: 0\n",
      "65 Metric score: 0\n",
      "66 Metric score: 0\n",
      "67 Metric score: 0\n",
      "68 Metric score: 0\n",
      "69 Metric score: 0\n",
      "70 Metric score: 0\n",
      "71 Metric score: 0\n",
      "72 Metric score: 0\n",
      "73 Metric score: 0\n",
      "74 Metric score: 0\n",
      "75 Metric score: 0\n",
      "76 Metric score: 0\n",
      "77 Metric score: 0\n",
      "78 Metric score: 0\n",
      "79 Metric score: 0\n",
      "80 Metric score: 0\n",
      "81 Metric score: 0\n",
      "82 Metric score: 0\n",
      "83 Metric score: 0\n",
      "84 Metric score: 0\n",
      "85 Metric score: 0\n",
      "86 Metric score: 0\n",
      "87 Metric score: 0\n",
      "88 Metric score: 0\n",
      "89 Metric score: 0\n",
      "90 Metric score: 0\n",
      "91 Metric score: 0\n",
      "92 Metric score: 0\n",
      "93 Metric score: 0\n",
      "94 Metric score: 0\n",
      "95 Metric score: 0\n",
      "96 Metric score: 0\n",
      "97 Metric score: 0\n",
      "98 Metric score: 0\n",
      "99 Metric score: 0\n",
      "100 Metric score: 0\n",
      "101 Metric score: 0\n",
      "102 Metric score: 0\n",
      "103 Metric score: 0\n",
      "104 Metric score: 0\n",
      "105 Metric score: 0\n",
      "106 Metric score: 0\n",
      "107 Metric score: 0\n",
      "108 Metric score: 0\n",
      "109 Metric score: 0\n",
      "110 Metric score: 0\n",
      "111 Metric score: 0\n",
      "112 Metric score: 0\n",
      "113 Metric score: 0\n",
      "114 Metric score: 0\n",
      "115 Metric score: 0\n",
      "116 Metric score: 0\n",
      "117 Metric score: 0\n",
      "118 Metric score: 0\n",
      "119 Metric score: 0\n",
      "120 Metric score: 0\n",
      "121 Metric score: 0\n",
      "122 Metric score: 0\n",
      "123 Metric score: 0\n",
      "124 Metric score: 0\n",
      "125 Metric score: 0\n",
      "126 Metric score: 0\n",
      "127 Metric score: 0\n",
      "128 Metric score: 0\n",
      "129 Metric score: 0\n",
      "130 Metric score: 0\n",
      "131 Metric score: 0\n",
      "132 Metric score: 0\n",
      "133 Metric score: 0\n",
      "134 Metric score: 0\n",
      "135 Metric score: 0\n",
      "136 Metric score: 0\n",
      "137 Metric score: 0\n",
      "138 Metric score: 0\n",
      "139 Metric score: 0\n",
      "140 Metric score: 0\n",
      "141 Metric score: 0\n",
      "142 Metric score: 0\n",
      "143 Metric score: 0\n",
      "144 Metric score: 0\n",
      "145 Metric score: 0\n",
      "146 Metric score: 0\n",
      "147 Metric score: 0\n",
      "148 Metric score: 0\n",
      "149 Metric score: 0\n",
      "150 Metric score: 0\n",
      "151 Metric score: 0\n",
      "152 Metric score: 0\n",
      "153 Metric score: 0\n",
      "154 Metric score: 0\n",
      "155 Metric score: 0\n",
      "156 Metric score: 0\n",
      "157 Metric score: 0\n",
      "158 Metric score: 0\n",
      "159 Metric score: 0\n",
      "160 Metric score: 0\n",
      "161 Metric score: 0\n",
      "162 Metric score: 0\n",
      "163 Metric score: 0\n",
      "164 Metric score: 0\n",
      "165 Metric score: 0\n",
      "166 Metric score: 0\n"
     ]
    }
   ],
   "source": [
    "metric_scores = []\n",
    "for i, data in enumerate(dataset):\n",
    "    if 0 < i < len(dataset):\n",
    "        try:\n",
    "            inp, target = data[\"text\"].split(\"[/INST]\")\n",
    "            prompt = inp + \"[/INST]\"\n",
    "            input_ids = fc_tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "            outputs = fc_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "            expected_str = data[\"text\"]\n",
    "            generated_str = fc_tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]\n",
    "\n",
    "            expected_data = parse_prompt_back_to_data(expected_str)\n",
    "            generated_data = parse_prompt_back_to_data(generated_str)\n",
    "\n",
    "            if \"function_call\" not in generated_data[\"target\"][\"chatgptMessage\"]:\n",
    "                metric_scores.append(0)\n",
    "                print(f\"{i} Metric score: {0}\")\n",
    "                continue\n",
    "            generated_arguments = json.loads(generated_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "            expected_arguments = json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "\n",
    "            metric_score = custom_metric(generated_arguments, expected_arguments)\n",
    "            metric_scores.append(metric_score)\n",
    "\n",
    "            print(f\"{i} Metric score: {metric_score:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "print(f\"Average metric score: {sum(metric_scores) / len(metric_scores):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
