{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c24bdeeb726a4b8c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\1seba\\.cache\\huggingface\\datasets\\json\\default-c24bdeeb726a4b8c\\0.0.0\\da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59e823a22064acc95ec81eadaad4af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca50e6d8e284818ad5dcb3638592256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd29aaf938094893b43e15ed525cfc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file 'C:\\Projects\\gaitor-function-calling\\evaluation\\production_eval_chat.json' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column() changed from object to array in row 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:108\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     pa_table \u001b[39m=\u001b[39m paj\u001b[39m.\u001b[39;49mread_json(\n\u001b[0;32m    109\u001b[0m         io\u001b[39m.\u001b[39;49mBytesIO(batch), read_options\u001b[39m=\u001b[39;49mpaj\u001b[39m.\u001b[39;49mReadOptions(block_size\u001b[39m=\u001b[39;49mblock_size)\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    111\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\pyarrow\\_json.pyx:259\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: JSON parse error: Column() changed from object to array in row 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\gaitor-function-calling\\evaluation\\evaluation.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/gaitor-function-calling/evaluation/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/gaitor-function-calling/evaluation/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m relative_path_to_data \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./production_eval_chat.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/gaitor-function-calling/evaluation/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m: relative_path_to_data}, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/gaitor-function-calling/evaluation/evaluation.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(dataset[\u001b[39m30\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\load.py:1679\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1676\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1678\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1680\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1681\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1682\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[0;32m   1683\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1684\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1685\u001b[0m )\n\u001b[0;32m   1687\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1689\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1690\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\builder.py:704\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[1;32m--> 704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    705\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager, verify_infos\u001b[39m=\u001b[39mverify_infos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\builder.py:793\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m    791\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    794\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    796\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    798\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    800\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\builder.py:1268\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator)\u001b[0m\n\u001b[0;32m   1266\u001b[0m generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_tables(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generator\u001b[39m.\u001b[39mgen_kwargs)\n\u001b[0;32m   1267\u001b[0m \u001b[39mwith\u001b[39;00m ArrowWriter(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures, path\u001b[39m=\u001b[39mfpath) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m-> 1268\u001b[0m     \u001b[39mfor\u001b[39;00m key, table \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   1269\u001b[0m         generator, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m tables\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, disable\u001b[39m=\u001b[39m(\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled())\n\u001b[0;32m   1270\u001b[0m     ):\n\u001b[0;32m   1271\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table)\n\u001b[0;32m   1272\u001b[0m     num_examples, num_bytes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\tqdm\\notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\MimicBot\\mimicbotwrapper\\env\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:136\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[0;32m    132\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNot able to read records in the JSON file at \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou should probably indicate the field of the JSON file containing your records. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis JSON file contain the following fields: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(\u001b[39mlist\u001b[39m(dataset\u001b[39m.\u001b[39mkeys()))\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSelect the correct one and provide it as `field=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mXXX\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` to the dataset loading method. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[39m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39myield\u001b[39;00m (file_idx, batch_idx), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cast_table(pa_table)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "relative_path_to_data = './production_eval_chat.json'\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "print(dataset[30][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "hub_id = \"SebastianS/function_calling-llama_7b\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "fc_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    hub_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "fc_tokenizer = AutoTokenizer.from_pretrained(hub_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "function_calling_tokens = {\n",
    "    \"FUNCTIONS\": {\n",
    "        \"start\": \"<FUNCTIONS>\",\n",
    "        \"end\": \"</FUNCTIONS>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_NAME\": {\n",
    "        \"start\": \"<FUNCTION_CALL_NAME>\",\n",
    "        \"end\": \"</FUNCTION_CALL_NAME>\"\n",
    "    },\n",
    "    \"FUNCTION_CALL_ARGUMENTS\": {\n",
    "        \"start\": \"<FUNCTION_CALL_ARGUMENTS>\",\n",
    "        \"end\": \"</FUNCTION_CALL_ARGUMENTS>\"\n",
    "    },\n",
    "    \"all\": [\"<FUNCTIONS>\", \"</FUNCTIONS>\", \"<FUNCTION_CALL_NAME>\", \"</FUNCTION_CALL_NAME>\", \"<FUNCTION_CALL_ARGUMENTS>\", \"</FUNCTION_CALL_ARGUMENTS>\"]\n",
    "}\n",
    "\n",
    "def parse_prompt_back_to_data(prompt):\n",
    "    \"\"\"\n",
    "    Function to parse a prompt back into the original data format, using a dictionary of function calling tokens.\n",
    "    \n",
    "    :param prompt: A string representing the constructed prompt.\n",
    "    :param function_calling_tokens: A dictionary containing the start and end tokens for different function call elements.\n",
    "    :return: A dictionary representing the original data instance.\n",
    "    \"\"\"\n",
    "    # Building regular expression patterns using the function_calling_tokens\n",
    "    functions_pattern = rf\"{function_calling_tokens['FUNCTIONS']['start']}(.*?){function_calling_tokens['FUNCTIONS']['end']}\"\n",
    "    input_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"  # This remains unchanged as it's not part of function_calling_tokens\n",
    "    target_content_pattern = r\"\\[/INST\\] (.*)</s>\"  # This also remains unchanged\n",
    "    function_call_name_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_NAME']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_NAME']['end']}\"\n",
    "    function_call_arguments_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}\"\n",
    "\n",
    "    # Extracting data using regular expressions\n",
    "    functions_str = re.search(functions_pattern, prompt).group(1)\n",
    "    input_content = re.search(input_pattern, prompt).group(1)\n",
    "    target_content_match = re.search(target_content_pattern, prompt)\n",
    "\n",
    "    # Parse functions JSON string\n",
    "    functions = json.loads(functions_str)\n",
    "\n",
    "    # Prepare the data dictionary\n",
    "    data = {\n",
    "        \"input\": [{\n",
    "            \"chatgptMessage\": {\"role\": \"user\", \"content\": input_content},\n",
    "            \"functions\": functions\n",
    "        }],\n",
    "        \"target\": {\n",
    "            \"chatgptMessage\": {\"role\": \"assistant\"},\n",
    "            \"functions\": functions  # Including functions in the target as well\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the target has a function call\n",
    "    if function_calling_tokens['FUNCTION_CALL_NAME']['start'] in prompt:\n",
    "        function_call_name = re.search(function_call_name_pattern, prompt).group(1)\n",
    "        function_call_arguments = re.search(function_call_arguments_pattern, prompt).group(1)\n",
    "        data[\"target\"][\"chatgptMessage\"][\"function_call\"] = {\n",
    "            \"name\": function_call_name,\n",
    "            \"arguments\": function_call_arguments\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where regex might not find a match for target content\n",
    "        if target_content_match:\n",
    "            target_content = target_content_match.group(1)\n",
    "            data[\"target\"][\"chatgptMessage\"][\"content\"] = target_content\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "embedding_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embedding_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "\n",
    "    return mean_pooled[0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    def compare_json(g_json, e_json, key_similarity_scores, value_similarity_scores):\n",
    "        for e_key, e_value in e_json.items():\n",
    "            # Check for exact key match or find the most similar key\n",
    "            if e_key in g_json:\n",
    "                g_key = e_key\n",
    "                key_similarity_scores.append(1)\n",
    "            else:\n",
    "                # Compute similarity with all keys in generated_json and find the best match\n",
    "                key_similarity = {gen_key: sentence_similarity(e_key, gen_key) for gen_key in g_json.keys()}\n",
    "                g_key, key_sim_score = max(key_similarity.items(), key=lambda x: x[1])\n",
    "                key_similarity_scores.append(key_sim_score)\n",
    "\n",
    "            # Recursive comparison for nested objects, else compare values\n",
    "            if isinstance(e_value, dict) and isinstance(g_json.get(g_key, {}), dict):\n",
    "                compare_json(g_json[g_key], e_value, key_similarity_scores, value_similarity_scores)\n",
    "            elif isinstance(e_value, str) and isinstance(g_json.get(g_key, \"\"), str):\n",
    "                # Compare values only if they are strings at the root level\n",
    "                value_sim_score = sentence_similarity(e_value, g_json[g_key])\n",
    "                value_similarity_scores.append(value_sim_score)\n",
    "            elif e_value == g_json.get(g_key, None):\n",
    "                value_similarity_scores.append(1)  # Exact match for non-string root values\n",
    "            else:\n",
    "                value_similarity_scores.append(0)  # Non-matching root values\n",
    "\n",
    "    key_similarity_scores = []\n",
    "    value_similarity_scores = []\n",
    "    compare_json(generated_json, expected_json, key_similarity_scores, value_similarity_scores)\n",
    "\n",
    "    # Compute the average similarity scores\n",
    "    avg_key_similarity = sum(key_similarity_scores) / len(key_similarity_scores) if key_similarity_scores else 0\n",
    "    avg_value_similarity = sum(value_similarity_scores) / len(value_similarity_scores) if value_similarity_scores else 0\n",
    "\n",
    "    return (avg_key_similarity + avg_value_similarity) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = []\n",
    "for data in dataset:\n",
    "    input_ids = fc_tokenizer(data[\"text\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = fc_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "    expected_str = data[\"text\"]\n",
    "    generated_str = fc_tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]\n",
    "\n",
    "    expected_data = parse_prompt_back_to_data(expected_str)\n",
    "    generated_data = parse_prompt_back_to_data(generated_str)\n",
    "\n",
    "    generated_arguments = json.loads(generated_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "    expected_arguments = json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "    \n",
    "    metric_score = custom_metric(generated_arguments, expected_arguments)\n",
    "    metric_scores.append(metric_score)\n",
    "\n",
    "    print(f\"Metric score: {metric_score:.2f}\")\n",
    "\n",
    "print(f\"Average metric score: {sum(metric_scores) / len(metric_scores):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
