{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/sosa.s/.cache/huggingface/datasets/json/default-8cefb13403ee4f92/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "<s>[INST] <<SYS>>\n",
      "Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. \n",
      "<FUNCTIONS>[{\"name\": \"getHashtagFollowerCount\", \"description\": \"Get the follower count, follower count over time and related trending hashtags of a specified LinkedIn hashtag\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path_params\": {\"type\": \"object\", \"properties\": {\"hashtag\": {\"type\": \"string\", \"description\": \"The specified hashtag.\"}}, \"required\": [\"hashtag\"]}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "linkedin hashtags for tech [/INST] <FUNCTION_CALL_NAME>getHashtagFollowerCount</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{}</FUNCTION_CALL_ARGUMENTS> </s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy\n",
    "relative_path_to_data = './production_eval_chat-instruction.json'\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "print(len(dataset))\n",
    "print(dataset[0][\"text\"])\n",
    "instruction = f\"Your job is to identify weather or not the user input is related to the function specification delimited by {function_calling_tokens['FUNCTIONS']['start']} and {function_calling_tokens['FUNCTIONS']['end']}. If it is related then your response should be in the function_calling format: {function_calling_tokens['FUNCTION_CALL_NAME']['start']}NAME_ASSOCIATED_WITH_THE_FUNCTION{function_calling_tokens['FUNCTION_CALL_NAME']['end']}{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}. Otherwise simply return a normal response. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2342d8d93bc4580beac8cfcd109f205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/SebastianS/function_calling-llama_7b-nat-fc_only/resolve/main/adapter_model.safetensors (Request ID: Root=1-655d1c02-248c587f63973cf649b1ceb6;a791c996-e0f8-4679-bba9-64568a0ffd0c)\n\nInternal Error - We're working hard to fix this as soon as possible!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/SebastianS/function_calling-llama_7b-nat-fc_only/resolve/main/adapter_model.safetensors",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m hub_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSebastianS/function_calling-llama_7b-nat-fc_only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# load base LLM model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m fc_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhub_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m fc_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(hub_id)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/auto.py:103\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m base_model \u001b[38;5;241m=\u001b[39m target_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_peft_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:271\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 271\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:529\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     use_safetensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     has_remote_safetensors_file \u001b[38;5;241m=\u001b[39m \u001b[43mhub_file_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSAFETENSORS_WEIGHTS_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     use_safetensors \u001b[38;5;241m=\u001b[39m has_remote_safetensors_file\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_remote_safetensors_file:\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# Priority 1: load safetensors weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/utils/hub_utils.py:26\u001b[0m, in \u001b[0;36mhub_file_exists\u001b[0;34m(repo_id, filename, revision, repo_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m url \u001b[38;5;241m=\u001b[39m hf_hub_url(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, filename\u001b[38;5;241m=\u001b[39mfilename, repo_type\u001b[38;5;241m=\u001b[39mrepo_type, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1608\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1600\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1601\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[0;32m-> 1608\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HfFileMetadata(\n\u001b[1;32m   1612\u001b[0m     commit_hash\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT),\n\u001b[1;32m   1613\u001b[0m     etag\u001b[38;5;241m=\u001b[39m_normalize_etag(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     size\u001b[38;5;241m=\u001b[39m_int_or_none(r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_LINKED_SIZE) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m   1624\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:303\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/SebastianS/function_calling-llama_7b-nat-fc_only/resolve/main/adapter_model.safetensors (Request ID: Root=1-655d1c02-248c587f63973cf649b1ceb6;a791c996-e0f8-4679-bba9-64568a0ffd0c)\n\nInternal Error - We're working hard to fix this as soon as possible!"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "hub_id = \"SebastianS/function_calling-llama_7b-nat-fc_only\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "fc_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    hub_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "fc_tokenizer = AutoTokenizer.from_pretrained(hub_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "embedding_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "def parse_prompt_back_to_data(prompt, instruction):\n",
    "    \"\"\"\n",
    "    Function to parse a prompt back into the original data format, using a dictionary of function calling tokens.\n",
    "    \n",
    "    :param prompt: A string representing the constructed prompt.\n",
    "    :param function_calling_tokens: A dictionary containing the start and end tokens for different function call elements.\n",
    "    :return: A dictionary representing the original data instance.\n",
    "    \"\"\"\n",
    "    # Building regular expression patterns using the function_calling_tokens\n",
    "    functions_pattern = rf\"{function_calling_tokens['FUNCTIONS']['start']}(.*?){function_calling_tokens['FUNCTIONS']['end']}\"\n",
    "    input_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"  # This remains unchanged as it's not part of function_calling_tokens\n",
    "    target_content_pattern = r\"\\[/INST\\] (.*)</s>\"  # This also remains unchanged\n",
    "    function_call_name_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_NAME']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_NAME']['end']}\"\n",
    "    function_call_arguments_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}\"\n",
    "\n",
    "    instruction_adjusted_prompt = prompt\n",
    "    if instruction:\n",
    "        instruction_adjusted_prompt = \"\".join(prompt.split(instruction))\n",
    "        \n",
    "    # Extracting data using regular expressions\n",
    "    functions_str = re.search(functions_pattern, instruction_adjusted_prompt).group(1)\n",
    "    input_content = re.search(input_pattern, prompt).group(1)\n",
    "    target_content_match = re.search(target_content_pattern, prompt)\n",
    "\n",
    "    # Parse functions JSON string\n",
    "    functions = json.loads(functions_str)\n",
    "\n",
    "    # Prepare the data dictionary\n",
    "    data = {\n",
    "        \"input\": [{\n",
    "            \"chatgptMessage\": {\"role\": \"user\", \"content\": input_content},\n",
    "            \"functions\": functions\n",
    "        }],\n",
    "        \"target\": {\n",
    "            \"chatgptMessage\": {\"role\": \"assistant\"},\n",
    "            \"functions\": functions  # Including functions in the target as well\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the target has a function call\n",
    "    if function_calling_tokens['FUNCTION_CALL_NAME']['start'] in prompt:\n",
    "        function_call_name = re.search(function_call_name_pattern, instruction_adjusted_prompt).group(1)\n",
    "        function_call_arguments = re.search(function_call_arguments_pattern, instruction_adjusted_prompt).group(1)\n",
    "        data[\"target\"][\"chatgptMessage\"][\"function_call\"] = {\n",
    "            \"name\": function_call_name,\n",
    "            \"arguments\": function_call_arguments\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where regex might not find a match for target content\n",
    "        if target_content_match:\n",
    "            target_content = target_content_match.group(1)\n",
    "            data[\"target\"][\"chatgptMessage\"][\"content\"] = target_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embedding_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "\n",
    "    return mean_pooled[0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    def compare_json(g_json, e_json, key_similarity_scores, value_similarity_scores):\n",
    "        for e_key, e_value in e_json.items():\n",
    "            # Check for exact key match or find the most similar key\n",
    "            if e_key in g_json:\n",
    "                g_key = e_key\n",
    "                key_similarity_scores.append(1)\n",
    "            else:\n",
    "                # Compute similarity with all keys in generated_json and find the best match\n",
    "                key_similarity = {gen_key: sentence_similarity(e_key, gen_key) for gen_key in g_json.keys()}\n",
    "                g_key, key_sim_score = max(key_similarity.items(), key=lambda x: x[1])\n",
    "                key_similarity_scores.append(key_sim_score)\n",
    "\n",
    "            # Recursive comparison for nested objects, else compare values\n",
    "            if isinstance(e_value, dict) and isinstance(g_json.get(g_key, {}), dict):\n",
    "                compare_json(g_json[g_key], e_value, key_similarity_scores, value_similarity_scores)\n",
    "            elif isinstance(e_value, str) and isinstance(g_json.get(g_key, \"\"), str):\n",
    "                # Compare values only if they are strings at the root level\n",
    "                value_sim_score = sentence_similarity(e_value, g_json[g_key])\n",
    "                value_similarity_scores.append(value_sim_score)\n",
    "            elif e_value == g_json.get(g_key, None):\n",
    "                value_similarity_scores.append(1)  # Exact match for non-string root values\n",
    "            else:\n",
    "                value_similarity_scores.append(0)  # Non-matching root values\n",
    "\n",
    "    key_similarity_scores = []\n",
    "    value_similarity_scores = []\n",
    "    compare_json(generated_json, expected_json, key_similarity_scores, value_similarity_scores)\n",
    "\n",
    "    # Compute the average similarity scores\n",
    "    avg_key_similarity = sum(key_similarity_scores) / len(key_similarity_scores) if key_similarity_scores else 0\n",
    "    avg_value_similarity = sum(value_similarity_scores) / len(value_similarity_scores) if value_similarity_scores else 0\n",
    "\n",
    "    return (avg_key_similarity + avg_value_similarity) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Metric score: 0.67\n",
      "2 Metric score: 1.00\n",
      "3 Metric score: 1.00\n",
      "4 Metric score: 1.00\n",
      "5 Metric score: 0.91\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "7 Metric score: 0.61\n",
      "8 Metric score: 0.82\n",
      "Error:  Expecting ',' delimiter: line 1 column 54 (char 53)\n",
      "10 Metric score: 1.00\n",
      "11 Metric score: 0.62\n",
      "12 Metric score: 0.00\n",
      "13 Metric score: 0.76\n",
      "14 Metric score: 0.92\n",
      "15 Metric score: 0.71\n",
      "16 Metric score: 0.53\n",
      "17 Metric score: 0.66\n",
      "18 Metric score: 1.00\n",
      "19 Metric score: 0.00\n",
      "20 Metric score: 0.60\n",
      "21 Metric score: 1.00\n",
      "22 Metric score: 1.00\n",
      "23 Metric score: 0.83\n",
      "24 Metric score: 0.79\n",
      "25 Metric score: 0.75\n",
      "26 Metric score: 0.00\n",
      "27 Metric score: 0.52\n",
      "28 Metric score: 0.63\n",
      "29 Metric score: 0.51\n",
      "30 Metric score: 0.43\n",
      "31 Metric score: 0.45\n",
      "32 Metric score: 0.46\n",
      "33 Metric score: 1.00\n",
      "34 Metric score: 0.63\n",
      "35 Metric score: 0.00\n",
      "36 Metric score: 0.83\n",
      "37 Metric score: 0.54\n",
      "38 Metric score: 0.61\n",
      "39 Metric score: 0.65\n",
      "40 Metric score: 0.00\n",
      "41 Metric score: 0.52\n",
      "42 Metric score: 0.76\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "45 Metric score: 0.88\n",
      "46 Metric score: 1.00\n",
      "47 Metric score: 0.58\n",
      "48 Metric score: 1.00\n",
      "49 Metric score: 1.00\n",
      "Error:  Expecting ',' delimiter: line 1 column 56 (char 55)\n",
      "51 Metric score: 0.78\n",
      "52 Metric score: 0.00\n",
      "Error:  max() arg is an empty sequence\n",
      "54 Metric score: 1.00\n",
      "55 Metric score: 0.00\n",
      "56 Metric score: 0.77\n",
      "57 Metric score: 0.63\n",
      "58 Metric score: 0.85\n",
      "59 Metric score: 0.47\n",
      "60 Metric score: 1.00\n",
      "61 Metric score: 0.63\n",
      "62 Metric score: 0.63\n",
      "63 Metric score: 0.79\n",
      "64 Metric score: 0.60\n",
      "Error:  max() arg is an empty sequence\n",
      "66 Metric score: 0.65\n",
      "67 Metric score: 0.48\n",
      "68 Metric score: 0.61\n",
      "69 Metric score: 0.61\n",
      "70 Metric score: 0.69\n",
      "71 Metric score: 0.61\n",
      "72 Metric score: 0.00\n",
      "73 Metric score: 0.00\n",
      "74 Metric score: 0.61\n",
      "75 Metric score: 0.63\n",
      "76 Metric score: 0.63\n",
      "77 Metric score: 0.57\n",
      "78 Metric score: 0.00\n",
      "79 Metric score: 0.88\n",
      "80 Metric score: 1.00\n",
      "81 Metric score: 0.00\n",
      "Error:  max() arg is an empty sequence\n",
      "83 Metric score: 0.60\n",
      "84 Metric score: 0.58\n",
      "85 Metric score: 1.00\n",
      "86 Metric score: 0.65\n",
      "87 Metric score: 0.36\n"
     ]
    }
   ],
   "source": [
    "metric_scores = []\n",
    "for i, data in enumerate(dataset):\n",
    "    if 0 < i < len(dataset):\n",
    "        try:\n",
    "            inp, target = data[\"text\"].split(\"[/INST]\")\n",
    "            prompt = inp + \"[/INST]\"\n",
    "            input_ids = fc_tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "            outputs = fc_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "    \n",
    "            expected_str = data[\"text\"]\n",
    "            generated_str = fc_tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]\n",
    "            \n",
    "    \n",
    "            expected_data = parse_prompt_back_to_data(expected_str, instruction)\n",
    "            generated_data = parse_prompt_back_to_data(generated_str, instruction)\n",
    "            parse_prompt_back_to_data(generated_str, instruction)\n",
    "    \n",
    "            if \"function_call\" not in generated_data[\"target\"][\"chatgptMessage\"]:\n",
    "                metric_scores.append(0)\n",
    "                print(f\"{i} Metric score: {0}\")\n",
    "                continue\n",
    "            generated_arguments = json.loads(generated_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "            expected_arguments = json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "    \n",
    "            metric_score = custom_metric(generated_arguments, expected_arguments)\n",
    "            metric_scores.append(metric_score)\n",
    "    \n",
    "            print(f\"{i} Metric score: {metric_score:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "print(f\"Average metric score: {sum(metric_scores) / len(metric_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
