{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/sosa.s/.cache/huggingface/datasets/json/default-8cefb13403ee4f92/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "<s>[INST] <<SYS>>\n",
      "Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. \n",
      "<FUNCTIONS>[{\"name\": \"getHashtagFollowerCount\", \"description\": \"Get the follower count, follower count over time and related trending hashtags of a specified LinkedIn hashtag\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path_params\": {\"type\": \"object\", \"properties\": {\"hashtag\": {\"type\": \"string\", \"description\": \"The specified hashtag.\"}}, \"required\": [\"hashtag\"]}}}}]</FUNCTIONS>\n",
      "<</SYS>>\n",
      "\n",
      "linkedin hashtags for tech [/INST] <FUNCTION_CALL_NAME>getHashtagFollowerCount</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{}</FUNCTION_CALL_ARGUMENTS> </s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy\n",
    "relative_path_to_data = './production_eval_chat-instruction.json'\n",
    "dataset = load_dataset('json', data_files={'train': relative_path_to_data}, split=\"train\")\n",
    "print(len(dataset))\n",
    "print(dataset[0][\"text\"])\n",
    "instruction = f\"Your job is to identify weather or not the user input is related to the function specification delimited by {function_calling_tokens['FUNCTIONS']['start']} and {function_calling_tokens['FUNCTIONS']['end']}. If it is related then your response should be in the function_calling format: {function_calling_tokens['FUNCTION_CALL_NAME']['start']}NAME_ASSOCIATED_WITH_THE_FUNCTION{function_calling_tokens['FUNCTION_CALL_NAME']['end']}{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}. Otherwise simply return a normal response. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6895ff5ab44c4017ab1d4936e011db55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint directory\n",
    "hub_id = \"SebastianS/function_calling-llama_7b-nat-fc_only\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "fc_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    hub_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "fc_tokenizer = AutoTokenizer.from_pretrained(hub_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load a pre-trained model for sentence embedding (e.g., SBERT)\n",
    "embedding_model_name = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "def parse_prompt_back_to_data(prompt, instruction):\n",
    "    \"\"\"\n",
    "    Function to parse a prompt back into the original data format, using a dictionary of function calling tokens.\n",
    "    \n",
    "    :param prompt: A string representing the constructed prompt.\n",
    "    :param function_calling_tokens: A dictionary containing the start and end tokens for different function call elements.\n",
    "    :return: A dictionary representing the original data instance.\n",
    "    \"\"\"\n",
    "    # Building regular expression patterns using the function_calling_tokens\n",
    "    functions_pattern = rf\"{function_calling_tokens['FUNCTIONS']['start']}(.*?){function_calling_tokens['FUNCTIONS']['end']}\"\n",
    "    input_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"  # This remains unchanged as it's not part of function_calling_tokens\n",
    "    target_content_pattern = r\"\\[/INST\\] (.*)</s>\"  # This also remains unchanged\n",
    "    function_call_name_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_NAME']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_NAME']['end']}\"\n",
    "    function_call_arguments_pattern = rf\"{function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['start']}(.*?){function_calling_tokens['FUNCTION_CALL_ARGUMENTS']['end']}\"\n",
    "\n",
    "    instruction_adjusted_prompt = prompt\n",
    "    if instruction:\n",
    "        instruction_adjusted_prompt = \"\".join(prompt.split(instruction))\n",
    "        \n",
    "    # Extracting data using regular expressions\n",
    "    functions_str = re.search(functions_pattern, instruction_adjusted_prompt).group(1)\n",
    "    input_content = re.search(input_pattern, prompt).group(1)\n",
    "    target_content_match = re.search(target_content_pattern, prompt)\n",
    "\n",
    "    # Parse functions JSON string\n",
    "    functions = json.loads(functions_str)\n",
    "\n",
    "    # Prepare the data dictionary\n",
    "    data = {\n",
    "        \"input\": [{\n",
    "            \"chatgptMessage\": {\"role\": \"user\", \"content\": input_content},\n",
    "            \"functions\": functions\n",
    "        }],\n",
    "        \"target\": {\n",
    "            \"chatgptMessage\": {\"role\": \"assistant\"},\n",
    "            \"functions\": functions  # Including functions in the target as well\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if the target has a function call\n",
    "    if function_calling_tokens['FUNCTION_CALL_NAME']['start'] in prompt:\n",
    "        function_call_name = re.search(function_call_name_pattern, instruction_adjusted_prompt).group(1)\n",
    "        function_call_arguments = re.search(function_call_arguments_pattern, instruction_adjusted_prompt).group(1)\n",
    "        data[\"target\"][\"chatgptMessage\"][\"function_call\"] = {\n",
    "            \"name\": function_call_name,\n",
    "            \"arguments\": function_call_arguments\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where regex might not find a match for target content\n",
    "        if target_content_match:\n",
    "            target_content = target_content_match.group(1)\n",
    "            data[\"target\"][\"chatgptMessage\"][\"content\"] = target_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = embedding_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_pooled = sum_embeddings / sum_mask\n",
    "\n",
    "    return mean_pooled[0].numpy()\n",
    "\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embedding1 = get_sentence_embedding(sent1)\n",
    "    embedding2 = get_sentence_embedding(sent2)\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "def custom_metric(generated_json, expected_json):\n",
    "    def compare_json(g_json, e_json, key_similarity_scores, value_similarity_scores):\n",
    "        for e_key, e_value in e_json.items():\n",
    "            # Check for exact key match or find the most similar key\n",
    "            if e_key in g_json:\n",
    "                g_key = e_key\n",
    "                key_similarity_scores.append(1)\n",
    "            else:\n",
    "                # Compute similarity with all keys in generated_json and find the best match\n",
    "                key_similarity = {gen_key: sentence_similarity(e_key, gen_key) for gen_key in g_json.keys()}\n",
    "                g_key, key_sim_score = max(key_similarity.items(), key=lambda x: x[1])\n",
    "                key_similarity_scores.append(key_sim_score)\n",
    "\n",
    "            # Recursive comparison for nested objects, else compare values\n",
    "            if isinstance(e_value, dict) and isinstance(g_json.get(g_key, {}), dict):\n",
    "                compare_json(g_json[g_key], e_value, key_similarity_scores, value_similarity_scores)\n",
    "            elif isinstance(e_value, str) and isinstance(g_json.get(g_key, \"\"), str):\n",
    "                # Compare values only if they are strings at the root level\n",
    "                value_sim_score = sentence_similarity(e_value, g_json[g_key])\n",
    "                value_similarity_scores.append(value_sim_score)\n",
    "            elif e_value == g_json.get(g_key, None):\n",
    "                value_similarity_scores.append(1)  # Exact match for non-string root values\n",
    "            else:\n",
    "                value_similarity_scores.append(0)  # Non-matching root values\n",
    "\n",
    "    key_similarity_scores = []\n",
    "    value_similarity_scores = []\n",
    "    compare_json(generated_json, expected_json, key_similarity_scores, value_similarity_scores)\n",
    "\n",
    "    # Compute the average similarity scores\n",
    "    avg_key_similarity = sum(key_similarity_scores) / len(key_similarity_scores) if key_similarity_scores else 0\n",
    "    avg_value_similarity = sum(value_similarity_scores) / len(value_similarity_scores) if value_similarity_scores else 0\n",
    "\n",
    "    return (avg_key_similarity + avg_value_similarity) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Metric score: 0.67\n",
      "2 Metric score: 1.00\n",
      "3 Metric score: 1.00\n",
      "4 Metric score: 1.00\n",
      "5 Metric score: 0.91\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "7 Metric score: 0.61\n",
      "8 Metric score: 0.82\n",
      "Error:  Expecting ',' delimiter: line 1 column 54 (char 53)\n",
      "10 Metric score: 1.00\n",
      "11 Metric score: 0.62\n",
      "12 Metric score: 0.00\n",
      "13 Metric score: 0.76\n",
      "14 Metric score: 0.92\n",
      "15 Metric score: 0.71\n",
      "16 Metric score: 0.53\n",
      "17 Metric score: 0.66\n",
      "18 Metric score: 1.00\n",
      "19 Metric score: 0.00\n",
      "20 Metric score: 0.60\n",
      "21 Metric score: 1.00\n",
      "22 Metric score: 1.00\n",
      "23 Metric score: 0.83\n",
      "24 Metric score: 0.79\n",
      "25 Metric score: 0.75\n",
      "26 Metric score: 0.00\n",
      "27 Metric score: 0.52\n",
      "28 Metric score: 0.63\n",
      "29 Metric score: 0.51\n",
      "30 Metric score: 0.43\n",
      "31 Metric score: 0.45\n",
      "32 Metric score: 0.46\n",
      "33 Metric score: 1.00\n",
      "34 Metric score: 0.63\n",
      "35 Metric score: 0.00\n",
      "36 Metric score: 0.83\n",
      "37 Metric score: 0.54\n",
      "38 Metric score: 0.61\n",
      "39 Metric score: 0.65\n",
      "40 Metric score: 0.00\n",
      "41 Metric score: 0.52\n",
      "42 Metric score: 0.76\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "Error:  'NoneType' object has no attribute 'group'\n",
      "45 Metric score: 0.88\n",
      "46 Metric score: 1.00\n",
      "47 Metric score: 0.58\n",
      "48 Metric score: 1.00\n",
      "49 Metric score: 1.00\n",
      "Error:  Expecting ',' delimiter: line 1 column 56 (char 55)\n",
      "51 Metric score: 0.78\n",
      "52 Metric score: 0.00\n",
      "Error:  max() arg is an empty sequence\n",
      "54 Metric score: 1.00\n",
      "55 Metric score: 0.00\n",
      "56 Metric score: 0.77\n",
      "57 Metric score: 0.63\n",
      "58 Metric score: 0.85\n",
      "59 Metric score: 0.47\n",
      "60 Metric score: 1.00\n",
      "61 Metric score: 0.63\n",
      "62 Metric score: 0.63\n",
      "63 Metric score: 0.79\n",
      "64 Metric score: 0.60\n",
      "Error:  max() arg is an empty sequence\n",
      "66 Metric score: 0.65\n",
      "67 Metric score: 0.48\n",
      "68 Metric score: 0.61\n",
      "69 Metric score: 0.61\n",
      "70 Metric score: 0.69\n",
      "71 Metric score: 0.61\n",
      "72 Metric score: 0.00\n",
      "73 Metric score: 0.00\n",
      "74 Metric score: 0.61\n",
      "75 Metric score: 0.63\n",
      "76 Metric score: 0.63\n",
      "77 Metric score: 0.57\n",
      "78 Metric score: 0.00\n",
      "79 Metric score: 0.88\n",
      "80 Metric score: 1.00\n",
      "81 Metric score: 0.00\n",
      "Error:  max() arg is an empty sequence\n",
      "83 Metric score: 0.60\n",
      "84 Metric score: 0.58\n",
      "85 Metric score: 1.00\n",
      "86 Metric score: 0.65\n",
      "87 Metric score: 0.36\n"
     ]
    }
   ],
   "source": [
    "metric_scores = []\n",
    "for i, data in enumerate(dataset):\n",
    "    if 0 < i < len(dataset):\n",
    "        try:\n",
    "            inp, target = data[\"text\"].split(\"[/INST]\")\n",
    "            prompt = inp + \"[/INST]\"\n",
    "            input_ids = fc_tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "            outputs = fc_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "    \n",
    "            expected_str = data[\"text\"]\n",
    "            generated_str = fc_tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]\n",
    "            \n",
    "    \n",
    "            expected_data = parse_prompt_back_to_data(expected_str, instruction)\n",
    "            generated_data = parse_prompt_back_to_data(generated_str, instruction)\n",
    "            parse_prompt_back_to_data(generated_str, instruction)\n",
    "    \n",
    "            if \"function_call\" not in generated_data[\"target\"][\"chatgptMessage\"]:\n",
    "                metric_scores.append(0)\n",
    "                print(f\"{i} Metric score: {0}\")\n",
    "                continue\n",
    "            generated_arguments = json.loads(generated_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "            expected_arguments = json.loads(expected_data[\"target\"][\"chatgptMessage\"][\"function_call\"][\"arguments\"])\n",
    "    \n",
    "            metric_score = custom_metric(generated_arguments, expected_arguments)\n",
    "            metric_scores.append(metric_score)\n",
    "    \n",
    "            print(f\"{i} Metric score: {metric_score:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "\n",
    "print(f\"Average metric score: {sum(metric_scores) / len(metric_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
