{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091e1455-6621-4198-a135-886bf1950aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.35.2 in /home/sosa.s/.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets==2.15.0 in /home/sosa.s/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: peft==0.6.2 in /home/sosa.s/.local/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: trl==0.7.4 in /home/sosa.s/.local/lib/python3.10/site-packages (0.7.4)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.35.2) (2023.10.3)\n",
      "Requirement already satisfied: requests in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (2.29.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from transformers==4.35.2) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (14.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/sosa.s/.local/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from datasets==2.15.0) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from datasets==2.15.0) (3.8.6)\n",
      "Requirement already satisfied: psutil in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from peft==0.6.2) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from peft==0.6.2) (2.0.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from peft==0.6.2) (0.23.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/sosa.s/.local/lib/python3.10/site-packages (from trl==0.7.4) (0.5.18)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.35.2) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.2) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.2) (3.1.2)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.4) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.4) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/sosa.s/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.4) (1.6.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.6.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.6.2) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.4) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.35.2\" \"datasets==2.15.0\" \"peft==0.6.2\" \"trl==0.7.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afb05ca-9182-4431-b115-0dcae7ddee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(project_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3ec341-4f18-464c-9abd-9177aed4da13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sosa.s/gaitor_function_calling'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816ced0e-2f54-4758-a41b-2f1ed6726e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "No raw data found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 833\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files='./data/train_test/glaive_dataset-only_fc-train-glaive-full_train/train.json')\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(10000))\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# get data and convert to Datasets format\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from gaitor_function_calling.data.data_utils import DataAbstractor\n",
    "from gaitor_function_calling.data.prompting_utils import INSTRUCTION\n",
    "\n",
    "data_abstractor = DataAbstractor(\"production_data-without_summary-only_fc.json\", \"sft\")\n",
    "\n",
    "train_df = pd.DataFrame(data_abstractor.train_data)\n",
    "test_df = pd.DataFrame(data_abstractor.test_data)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_sft = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e42a61-dd92-4978-be49-e78f5168456a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7a489c69454449a92991ce62ff156e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "output_dir=\"llama-7-chat-instruction-int4-glaive-fc-testing\"\n",
    "hub_model_id=f\"SebastianS/{output_dir}\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3689a2ac-10c0-4aa1-a436-1d9297c42ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7b3dcb3bc4510b670a0210e0f5114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True).to(torch.device(\"cuda\", index=3))\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3434be57-c368-4063-872d-ab41063296b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/17143770/ipykernel_291065/4091857241.py:92: FutureWarning: Metric is deprecated and will be removed in the next major version of datasets. Use the new library ü§ó Evaluate instead: https://huggingface.co/docs/evaluate\n",
      "  my_custom_metric = MyCustomMetric()\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tensor([[[-0.0523, -0.2910,  0.3679,  ...,  1.1621,  1.7275,  0.5503],\n",
      "         [-0.0523, -0.2910,  0.3679,  ...,  1.1621,  1.7275,  0.5503],\n",
      "         [-6.3398, -0.7319,  1.3848,  ..., -4.5820, -6.2188, -2.3750],\n",
      "         ...,\n",
      "         [ 0.0693,  1.1943, 20.2969,  ..., -0.1329, -0.6284,  0.1520],\n",
      "         [-0.2079, -2.1934, 16.8281,  ...,  1.4727, -1.4873, -1.9941],\n",
      "         [-2.6406,  1.3369, 10.1875,  ..., -3.5703, -4.8867, -2.0137]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "label_ids: tensor([[    1,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,  3575,\n",
      "          4982,   338,   304, 12439, 14826,   470,   451,   278,  1404,  1881,\n",
      "           338,  4475,   304,   278,   740, 21992,   628,   326,  1573,   491,\n",
      "           529, 29943, 28700, 29903, 29958,   322,  1533, 29943, 28700, 29903,\n",
      "         15513,   960,   372,   338,  4475,   769,   596,  2933,   881,   367,\n",
      "           297,   278,   740, 29918,  4804,   292,  3402, 29901,   529, 29943,\n",
      "         28700, 29918, 29907,  9818, 29918,  5813, 29958,  5813, 29918, 22933,\n",
      "         29949,  8426,  3040, 29928, 29918, 29956, 13054, 29918, 28350, 29918,\n",
      "         29943, 28700,   829, 29943, 28700, 29918, 29907,  9818, 29918,  5813,\n",
      "          5299, 29943, 28700, 29918, 29907,  9818, 29918,  1718, 29954,  5005,\n",
      "          3919, 29903, 29958,  1718, 29954,  5005,  3919, 29903, 29918,  1177,\n",
      "         29918, 20785, 29902,  3738,  3352, 29918,  7249, 29918, 19094,  1299,\n",
      "           829, 29943, 28700, 29918, 29907,  9818, 29918,  1718, 29954,  5005,\n",
      "          3919, 29903, 15513, 13466,  3763,   736,   263,  4226,  2933, 29889,\n",
      "           529, 29943, 28700, 29903, 24566,  6377,   978,  1115,   376,  4632,\n",
      "         29918,   354,  4298,  3198, 29918,   657,   613,   376,  8216,  1115,\n",
      "           376, 10303,   613,   376, 16744,  1115,  8853,  1853,  1115,   376,\n",
      "          3318,   613,   376, 11330,  1115,   426,   930,  1118,  8853,   978,\n",
      "          1115,   376,   657,  7974, 12191,   613,   376,  8216,  1115,   376,\n",
      "          2577,   263,  1051,   310,   330, 10270,  2729,   373,   278,  2740,\n",
      "          2346,   613,   376, 16744,  1115,  8853,  1853,  1115,   376,  3318,\n",
      "           613,   376, 11330,  1115,  8853,  3126,  1115,  8853, 11330,  1115,\n",
      "          8853,  1972,  1115,  8853,  1853,  1115,   376,  1807,   613,   376,\n",
      "          3257,  1115,   376,  3010, 29908, 11656,   376,  1853,  1115,   376,\n",
      "          3318,   613,   376, 12403,  1115,  6796,  1972, 12436,   376,  3257,\n",
      "          1115,   376, 26074, 11461,  3089, 29908,   930, 11656,  8853,   978,\n",
      "          1115,   376,   657,  2308,  2548, 12191,   613,   376,  8216,  1115,\n",
      "           376,  2577,   263,  1051,   310,   330, 10270,  2729,   373,   278,\n",
      "          1857,   534,  1975,   613,   376, 16744,  1115,  8853,  1853,  1115,\n",
      "           376,  3318,   613,   376, 11330,  1115,   426,   930,  6525,   829,\n",
      "         29943, 28700, 29903, 29958,   529,   829, 14816, 29903,  6778,  1510,\n",
      "           592,   697,   330,   361,   310,   263, 12528,  6635,   518, 29914,\n",
      "         25580, 29962,   529, 29943, 28700, 29918, 29907,  9818, 29918,  5813,\n",
      "         29958,   657,  7974, 12191,   829, 29943, 28700, 29918, 29907,  9818,\n",
      "         29918,  5813,  5299, 29943, 28700, 29918, 29907,  9818, 29918,  1718,\n",
      "         29954,  5005,  3919, 29903, 29958,  6377,  3126,  1115,  8853,  1972,\n",
      "          1115,   376,  1111,   324,  6635, 29908,   930,   829, 29943, 28700,\n",
      "         29918, 29907,  9818, 29918,  1718, 29954,  5005,  3919, 29903, 29958,\n",
      "         29871,     2]], device='cuda:0')\n",
      "\n",
      "\n",
      "predicted_strings:  ['Unterscheidung Unterscheidung![: What<<NOP What favorite is to help the patterns climate a following is is a to a topic you\\nineited by the >>_> in returnFUNCTIONS>\\n the is related, return program should be \" the format formatname format format. functionFUNCTION_CALL>FORM>( <</OFIGNCIATED_WITH_USER_USERUNCTION</FUNCTION_CALL_NAME>brUNCTIONSCALL_NAMEGSUMENTS>\\nGUMENTS_ASS_THE_FIED_FORM_FORMAT</FUNCTION_CALL_ARGUMENTS> If, respond \" \" response.\\nFUNCTIONS>username\": \"get\",meanalth\",\",function_ \"description\": \"Get health \"input\": [root\": \"string\", \"properties\": {\" \"}] {\"name\": \"leaf_Results\", \"description\": \"Search search list of searchists\", on a search query\", \"parameters\": {\"type\": \"array\", \"properties\": {\"query\": {\"type\": {\"query\": {\"type\": \"string\" \"required\": \"Search\"}} \"type\": \"array\" \"title\": [\"query\"] \"description\": \"Searchphy Search\"}}}, {\"name\": \"getUserending\",\", \"description\": \"Get a list of trifs that on the tr trending\", \"parameters\": {\"type\": \"object\", \"properties\": {\"}}},}</FUNCTIONS>\\nUSERFS>\\n me the ofif from a cat cat doing</users FUNCTION_CALL_NAME>rootSearchResults</FUNCTION_CALL_NAME>FUNCTION_CALL_ARGUMENTS>{\"json\": {\"query\": \"cool cat\"}}</FUNCTION_CALL_ARGUMENTS></s>ÔøΩÔøΩ']\n",
      "\n",
      "\n",
      "label_strings:  ['<s><s> [INST] <<SYS>> Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. <FUNCTIONS>[{\"name\": \"root_healthcheck_get\", \"description\": \"Root\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}, {\"name\": \"getSearchResults\", \"description\": \"Get a list of gifs based on the search query\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"query\": {\"type\": \"string\", \"title\": \"Query\"}}, \"type\": \"object\", \"required\": [\"query\"], \"title\": \"GiphyRequest\"}}}}, {\"name\": \"getTrendingResults\", \"description\": \"Get a list of gifs based on the current trends\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]</FUNCTIONS> <</SYS>> show me one gif of a cool cat [/INST] <FUNCTION_CALL_NAME>getSearchResults</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"query\": \"cool cat\"}}</FUNCTION_CALL_ARGUMENTS> </s>']\n",
      "\n",
      "\n",
      "Example 0: Error function calling: 'NoneType' object has no attribute 'group'\n",
      "\n",
      "{'my_custom_metric': [0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Metric\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from gaitor_function_calling.evaluation.evaluation_utils import FunctionCallingMetric, compute_perplexity, get_logits_and_labels\n",
    "from gaitor_function_calling.data.prompting_utils import INSTRUCTION, json_arguments_from_prompt\n",
    "import datasets\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "fc_metric = FunctionCallingMetric()\n",
    "epoch=-1\n",
    "class MyCustomMetric(Metric):\n",
    "    def _info(self):\n",
    "        # Returns the MetricInfo that defines the name, description, etc.\n",
    "        return datasets.MetricInfo(\n",
    "            # This should be a short description of your metric.\n",
    "            description=\"_DESCRIPTION\",\n",
    "            # You can cite papers, GitHub repositories, etc.\n",
    "            citation=\"_CITATION\",\n",
    "            # The inputs and outputs your metric expects.\n",
    "            # These are used to validate the inputs and outputs of _compute\n",
    "            inputs_description=\"_KWARGS_DESCRIPTION\",\n",
    "            features=datasets.Features({\n",
    "                'predictions': datasets.Value('string'),\n",
    "                'references': datasets.Value('string')\n",
    "            })\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        # Here is where you should put your main metric computation logic\n",
    "        # Adapt your existing code to fit in here\n",
    "        \n",
    "        fc_results = []\n",
    "        for idx, example in enumerate(predictions):\n",
    "            print(f\"Example {idx}: \", end=\"\")\n",
    "            post_message = \"\"\n",
    "\n",
    "            # Custom Function Calling metric\n",
    "            prompts = None\n",
    "            try:\n",
    "                generated_arguments, expected_arguments, prompts = json_arguments_from_prompt(\n",
    "                    references[idx],\n",
    "                    predictions[idx],\n",
    "                    INSTRUCTION\n",
    "                    # {\"idx\": idx, \"epoch\": epoch}\n",
    "                )\n",
    "                fc_result = fc_metric.run(generated_arguments, expected_arguments)\n",
    "\n",
    "                fc_results.append(fc_result)\n",
    "\n",
    "                # if save_prompts_path:\n",
    "                #     # add prompts to dpo_data.json\n",
    "                #     dpo_data.append({\n",
    "                #         \"fc_result\": fc_result,\n",
    "                #         **prompts\n",
    "                #     })\n",
    "                #     with open(save_prompts_path, \"w\") as f:\n",
    "                #         json.dump(dpo_data, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error function calling: {e}\\n\")\n",
    "                fc_results.append(0)\n",
    "        return fc_results\n",
    "\n",
    "    \n",
    "sample = {\n",
    "    \"fc_result\":1,\n",
    "    \"expected_str\":\"\"\"<s>[INST] <<SYS>> Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. <FUNCTIONS>[{\"name\": \"root_healthcheck_get\", \"description\": \"Root\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}, {\"name\": \"getSearchResults\", \"description\": \"Get a list of gifs based on the search query\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"query\": {\"type\": \"string\", \"title\": \"Query\"}}, \"type\": \"object\", \"required\": [\"query\"], \"title\": \"GiphyRequest\"}}}}, {\"name\": \"getTrendingResults\", \"description\": \"Get a list of gifs based on the current trends\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]</FUNCTIONS> <</SYS>> show me one gif of a cool cat [/INST] <FUNCTION_CALL_NAME>getSearchResults</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"query\": \"cool cat\"}}</FUNCTION_CALL_ARGUMENTS> </s>\"\"\",\n",
    "    \"generated_str\":\"\"\"<s><s> [INST] <<SYS>> Your job is to identify weather or not the user input is related to the function specification delimited by <FUNCTIONS> and </FUNCTIONS>. If it is related then your response should be in the function_calling format: <FUNCTION_CALL_NAME>NAME_ASSOCIATED_WITH_THE_FUNCTION</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>ARGUMENTS_IN_STRINGIFIED_JSON_FORMAT</FUNCTION_CALL_ARGUMENTS>. Otherwise simply return a normal response. <FUNCTIONS>[{\"name\": \"root_healthcheck_get\", \"description\": \"Root\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}, {\"name\": \"getSearchResults\", \"description\": \"Get a list of gifs based on the search query\", \"parameters\": {\"type\": \"object\", \"properties\": {\"json\": {\"properties\": {\"query\": {\"type\": \"string\", \"title\": \"Query\"}}, \"type\": \"object\", \"required\": [\"query\"], \"title\": \"GiphyRequest\"}}}}, {\"name\": \"getTrendingResults\", \"description\": \"Get a list of gifs based on the current trends\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]</FUNCTIONS> <</SYS>> show me one gif of a cool cat [/INST] <FUNCTION_CALL_NAME>getSearchResults</FUNCTION_CALL_NAME><FUNCTION_CALL_ARGUMENTS>{\"json\": {\"query\": \"cool cat\"}}</FUNCTION_CALL_ARGUMENTS> </s>\"\"\"\n",
    "}\n",
    "\n",
    "class TestMyCustomMetric():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metric = MyCustomMetric()\n",
    "\n",
    "    def test_simple_case(self):\n",
    "        # Test a simple case with known inputs and outputs\n",
    "        predictions = [sample[\"expected_str\"]]\n",
    "        references = [sample[\"generated_str\"]]\n",
    "        \n",
    "        expected_score = 1  # Replace with the expected score\n",
    "        result = self.metric._compute(predictions=predictions, references=references)\n",
    "        print(expected_score, result)\n",
    "\n",
    "    def test_edge_cases(self):\n",
    "        # Test various edge cases like empty inputs, very long texts, etc.\n",
    "        pass\n",
    "\n",
    "    # Add more test cases as needed\n",
    "\n",
    "my_custom_metric = MyCustomMetric()\n",
    "tester = TestMyCustomMetric()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predictions, label_ids = eval_pred\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = torch.from_numpy(eval_pred.predictions)\n",
    "        label_ids = torch.from_numpy(eval_pred.label_ids)\n",
    "    # You might need to adjust the format of predictions and labels\n",
    "    # to match what your custom metric expects\n",
    "    print(\"predictions:\",predictions, end=\"\\n\\n\\n\")\n",
    "    print(\"label_ids:\",label_ids, end=\"\\n\\n\\n\")\n",
    "    \n",
    "    predicted_token_ids = torch.argmax(predictions, dim=-1).to(device)\n",
    "    \n",
    "    # Replace -100 in label_ids with tokenizer.pad_token_id\n",
    "    label_ids = label_ids.clone()  # Clone to avoid modifying the original tensor\n",
    "    ignore_index_mask = label_ids == -100\n",
    "    label_ids[ignore_index_mask] = tokenizer.pad_token_id\n",
    "    \n",
    "    predicted_tokens = [tokenizer.decode(ids) for ids in predicted_token_ids]\n",
    "    label_tokens = [tokenizer.decode(ids) for ids in label_ids]\n",
    "\n",
    "    # Join tokens into strings\n",
    "    predicted_strings = [''.join(tokens) for tokens in predicted_tokens]\n",
    "    label_strings = [''.join(tokens) for tokens in label_tokens]\n",
    "    \n",
    "    print(\"predicted_strings: \", predicted_strings, end=\"\\n\\n\\n\")\n",
    "    print(\"label_strings: \", label_strings, end=\"\\n\\n\\n\")\n",
    "\n",
    "    # Compute the metric\n",
    "    metric_result = my_custom_metric._compute(predictions=predicted_strings, references=label_strings)\n",
    "    \n",
    "    # Return results in a dictionary\n",
    "    return {\"my_custom_metric\": metric_result}\n",
    "\n",
    "\n",
    "\n",
    "# Sample texts\n",
    "sample_texts = [sample[\"expected_str\"]]\n",
    "# Tokenize texts\n",
    "inputs = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate logits using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Create labels (shifting input_ids to the right by one)\n",
    "labels = inputs.input_ids\n",
    "\n",
    "# Create an EvalPrediction object\n",
    "mock_eval_pred = EvalPrediction(predictions=logits, label_ids=labels)\n",
    "\n",
    "# Now you can use mock_eval_pred to test your compute_metrics function\n",
    "result = compute_metrics(mock_eval_pred)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "# tester.test_simple_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "236438e2-9114-4d71-b836-7e23a819dae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('‚ñÅ[', 518), ('/', 29914), ('INST', 25580), (']', 29962)]\n",
      "[('<s>', 1), ('‚ñÅ', 29871), ('‚ñÅ[', 518), ('INST', 25580), (']', 29962), ('‚ñÅ<<', 3532), ('SY', 14816), ('S', 29903), ('>>', 6778)]\n"
     ]
    }
   ],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "response_template = \"[/INST]\"\n",
    "print_tokens_with_ids(response_template)\n",
    "instruction_template = \"<s> [INST] <<SYS>>\"\n",
    "print_tokens_with_ids(instruction_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff81f46-7c53-4508-b0f6-de6292f69bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=hub_model_id,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=3,\n",
    "    eval_steps=1000,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "response_template = \"[/INST]\"\n",
    "instruction_template = \"[INST]\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c8be1-2126-4a64-8691-a06d2fe915d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "044ff02d-0087-4f1e-b6a2-00912faf225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (`Lst[str]`, *optional*):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_sagemaker_mp_enabled():\n",
    "                raw_outputs = smp_forward_only(model, inputs)\n",
    "                if has_labels:\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        loss_mb = raw_outputs[\"loss\"]\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        loss_mb = raw_outputs[0]\n",
    "                        logits_mb = raw_outputs[1:]\n",
    "\n",
    "                    loss = loss_mb.reduce_mean().detach().cpu()\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "                else:\n",
    "                    loss = None\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits_mb = raw_outputs\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "            else:\n",
    "                if has_labels:\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                    loss = loss.mean().detach()\n",
    "\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        logits = outputs[1:]\n",
    "                else:\n",
    "                    loss = None\n",
    "                    with self.compute_loss_context_manager():\n",
    "                        outputs = model(**inputs)\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits = outputs\n",
    "                    # TODO: this needs to be fixed and made cleaner later.\n",
    "                    if self.args.past_index >= 0:\n",
    "                        self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243c8fce-1066-48f6-a8cc-033bbb73cf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa.s/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset_sft[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=4096,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefbe01-f673-4ac8-8403-15ca562fe477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
